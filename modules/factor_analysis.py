"""
ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Fama-French 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ + Momentumãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ
"""

import pandas as pd
import numpy as np
import yfinance as yf
try:
    import statsmodels.api as sm
    from statsmodels.regression.rolling import RollingOLS
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    # statsmodelsãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿å®Ÿè£…
    class MockModel:
        def __init__(self):
            self.params = pd.Series()
            self.pvalues = pd.Series()
            self.rsquared = 0
            self.rsquared_adj = 0
            self.fvalue = 0
            self.f_pvalue = 1
            self.resid = pd.Series()
            self.fittedvalues = pd.Series()

from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


def download_fama_french_direct(start_date: str, end_date: str) -> pd.DataFrame:
    """
    Kenneth Frenchå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå …ç‰¢ç‰ˆï¼‰
    
    Args:
        start_date: é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
        end_date: çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
    
    Returns:
        pd.DataFrame: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆMkt-RF, SMB, HML, RMW, CMA, Mom, RFï¼‰
    """
    import requests
    import zipfile
    import io
    import time
    from datetime import datetime
    
    logger.info("ğŸ¯ Kenneth Frenchå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
    
    # Kenneth Frenchå…¬å¼ã‚µã‚¤ãƒˆã®URLï¼ˆå …ç‰¢æ€§ã®ãŸã‚è¤‡æ•°ã®ãƒŸãƒ©ãƒ¼ï¼‰
    ff5_urls = [
        "https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_daily_CSV.zip",
        "https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_Daily_CSV.zip"
    ]
    
    mom_urls = [
        "https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_daily_CSV.zip",
        "https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_Daily_CSV.zip"
    ]
    
    # å …ç‰¢ãªHTTPã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    
    def robust_download_and_parse(urls, data_type, expected_columns):
        """å …ç‰¢ãªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ‘ãƒ¼ã‚¹"""
        for attempt, url in enumerate(urls):
            for retry in range(3):  # æœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤
                try:
                    logger.info(f"ğŸ“¥ {data_type}ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­... (URL {attempt+1}, è©¦è¡Œ {retry+1}/3)")
                    
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤è¨­å®š
                    timeout = 45 + (retry * 15)  # 45, 60, 75ç§’
                    response = session.get(url, timeout=timeout, stream=True)
                    response.raise_for_status()
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                    content_length = response.headers.get('content-length')
                    if content_length and int(content_length) < 1000:
                        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™: {content_length} bytes")
                    
                    # ZIPãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
                    zip_content = response.content
                    logger.info(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {len(zip_content)} bytes")
                    
                    with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                        # ZIPå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
                        file_list = zip_file.namelist()
                        logger.info(f"ZIPå†…ãƒ•ã‚¡ã‚¤ãƒ«: {file_list}")
                        
                        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                        csv_file = None
                        for filename in file_list:
                            if filename.lower().endswith('.csv'):
                                csv_file = filename
                                break
                        
                        if not csv_file:
                            raise ValueError(f"ZIPå†…ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_list}")
                        
                        logger.info(f"ğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­: {csv_file}")
                        
                        with zip_file.open(csv_file) as csv_data:
                            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
                            raw_content = csv_data.read()
                            
                            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
                            content = None
                            for encoding in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:
                                try:
                                    content = raw_content.decode(encoding)
                                    logger.info(f"âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {encoding}")
                                    break
                                except UnicodeDecodeError:
                                    continue
                            
                            if content is None:
                                raise ValueError("ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç‰¹å®šã§ãã¾ã›ã‚“")
                            
                            # Kenneth Frenchãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®è§£æï¼ˆå …ç‰¢ç‰ˆï¼‰
                            lines = content.split('\n')
                            logger.info(f"ğŸ“ ç·è¡Œæ•°: {len(lines)}")
                            
                            # ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œã®æ¤œç´¢ï¼ˆã‚ˆã‚Šå …ç‰¢ãªæ¤œç´¢ï¼‰
                            data_start = None
                            
                            # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œã‚’æ¤œç´¢
                            search_patterns = [
                                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: 8æ¡ã®æ•°å­—ã§å§‹ã¾ã‚‹è¡Œï¼ˆYYYYMMDDå½¢å¼ï¼‰
                                lambda line: (
                                    len(line.strip().split(',')) >= len(expected_columns) and
                                    line.strip().split(',')[0].strip().isdigit() and
                                    len(line.strip().split(',')[0].strip()) == 8 and
                                    int(line.strip().split(',')[0].strip()[:4]) >= 1900
                                ),
                                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚ˆã‚Šç·©ã„8æ¡æ•°å­—ãƒã‚§ãƒƒã‚¯
                                lambda line: (
                                    ',' in line and
                                    len(line.strip().split(',')) >= 3 and
                                    line.strip().split(',')[0].strip().isdigit() and
                                    len(line.strip().split(',')[0].strip()) == 8
                                ),
                                # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ•°å­—ã§å§‹ã¾ã‚Šã€ã‚«ãƒ³ãƒãŒè¤‡æ•°å«ã¾ã‚Œã‚‹è¡Œ
                                lambda line: (
                                    ',' in line and
                                    line.strip().split(',')[0].strip().isdigit() and
                                    len(line.strip().split(',')[0].strip()) >= 6 and
                                    line.count(',') >= 2
                                )
                            ]
                            
                            # ã‚¹ã‚­ãƒƒãƒ—ã™ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³
                            skip_patterns = [
                                'copyright', 'research', 'data', 'description', 'note',
                                'created', 'updated', 'source', 'french', 'fama',
                                'date', 'factor', 'portfolio', 'return', 'average',
                                'explanation', 'definition', 'construction'
                            ]
                            
                            # ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œã‚’æ®µéšçš„ã«æ¤œç´¢
                            for pattern_idx, pattern_func in enumerate(search_patterns):
                                logger.info(f"ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³{pattern_idx + 1}ã§ãƒ‡ãƒ¼ã‚¿è¡Œæ¤œç´¢ä¸­...")
                                
                                search_range = min(100, len(lines))  # æœ€åˆã®100è¡Œã‚’ãƒã‚§ãƒƒã‚¯
                                for i, line in enumerate(lines[:search_range]):
                                    line_stripped = line.strip()
                                    if not line_stripped:
                                        continue
                                    
                                    # ã‚¹ã‚­ãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
                                    if any(skip in line_stripped.lower() for skip in skip_patterns):
                                        continue
                                    
                                    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®å¯èƒ½æ€§ãŒã‚ã‚‹ã‚‚ã®ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                    if i < 20 and any(char.isalpha() for char in line_stripped[:10]):
                                        if not line_stripped.split(',')[0].strip().isdigit():
                                            continue
                                    
                                    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                                    try:
                                        if pattern_func(line_stripped):
                                            # è¿½åŠ æ¤œè¨¼ï¼šå®Ÿéš›ã«æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                            parts = line_stripped.split(',')
                                            if len(parts) >= len(expected_columns):
                                                # æ—¥ä»˜ä»¥å¤–ã®åˆ—ãŒæ•°å€¤ã‹ãƒã‚§ãƒƒã‚¯
                                                numeric_count = 0
                                                for j in range(1, min(len(parts), len(expected_columns))):
                                                    try:
                                                        float(parts[j].strip())
                                                        numeric_count += 1
                                                    except (ValueError, TypeError):
                                                        pass
                                                
                                                # å°‘ãªãã¨ã‚‚åŠåˆ†ã®åˆ—ãŒæ•°å€¤ãƒ‡ãƒ¼ã‚¿ãªã‚‰æœ‰åŠ¹ã¨ã™ã‚‹
                                                if numeric_count >= (len(expected_columns) - 1) // 2:
                                                    data_start = i
                                                    logger.info(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³{pattern_idx + 1}ã§ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œç™ºè¦‹: {i+1}è¡Œç›®")
                                                    logger.info(f"ğŸ“Š æ¤œè¨¼: {numeric_count}/{len(expected_columns)-1}åˆ—ãŒæ•°å€¤ãƒ‡ãƒ¼ã‚¿")
                                                    break
                                    except Exception as e:
                                        logger.debug(f"ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ï¼ˆè¡Œ{i+1}ï¼‰: {str(e)}")
                                        continue
                                
                                if data_start is not None:
                                    break
                            
                            if data_start is None:
                                logger.error("âŒ å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                logger.info("ğŸ” æœ€åˆã®20è¡Œã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›:")
                                for i, line in enumerate(lines[:20]):
                                    logger.info(f"  è¡Œ{i+1}: {line.strip()[:100]}")
                                raise ValueError("ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            
                            # ãƒ‡ãƒ¼ã‚¿è¡Œã®æŠ½å‡ºï¼ˆå …ç‰¢ç‰ˆï¼‰
                            data_lines = []
                            consecutive_invalid_lines = 0
                            max_consecutive_invalid = 50  # é€£ç¶šã§ç„¡åŠ¹ãªè¡ŒãŒ50è¡Œç¶šã„ãŸã‚‰çµ‚äº†
                            
                            logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹ï¼ˆé–‹å§‹è¡Œ: {data_start + 1}ï¼‰")
                            
                            for i, line in enumerate(lines[data_start:], start=data_start):
                                line_stripped = line.strip()
                                if not line_stripped:
                                    consecutive_invalid_lines += 1
                                    if consecutive_invalid_lines > max_consecutive_invalid:
                                        logger.info(f"ğŸ›‘ é€£ç¶šç©ºè¡ŒãŒ{max_consecutive_invalid}è¡Œç¶šã„ãŸãŸã‚çµ‚äº†")
                                        break
                                    continue
                                
                                if ',' not in line_stripped:
                                    consecutive_invalid_lines += 1
                                    if consecutive_invalid_lines > max_consecutive_invalid:
                                        logger.info(f"ğŸ›‘ é€£ç¶šç„¡åŠ¹è¡ŒãŒ{max_consecutive_invalid}è¡Œç¶šã„ãŸãŸã‚çµ‚äº†")
                                        break
                                    continue
                                
                                parts = [p.strip() for p in line_stripped.split(',')]
                                
                                # åˆ—æ•°ãƒã‚§ãƒƒã‚¯
                                if len(parts) < len(expected_columns):
                                    consecutive_invalid_lines += 1
                                    if consecutive_invalid_lines > max_consecutive_invalid:
                                        logger.info(f"ğŸ›‘ é€£ç¶šçŸ­è¡ŒãŒ{max_consecutive_invalid}è¡Œç¶šã„ãŸãŸã‚çµ‚äº†")
                                        break
                                    continue
                                
                                # æ—¥ä»˜å½¢å¼ã®æ¤œè¨¼ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
                                date_part = parts[0]
                                is_valid_date = False
                                
                                try:
                                    # 8æ¡æ•°å­—ã®æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
                                    if date_part.isdigit() and len(date_part) == 8:
                                        year = int(date_part[:4])
                                        month = int(date_part[4:6])
                                        day = int(date_part[6:8])
                                        
                                        if 1900 <= year <= 2030 and 1 <= month <= 12 and 1 <= day <= 31:
                                            is_valid_date = True
                                    # 6æ¡æ•°å­—ã®æ—¥ä»˜ãƒã‚§ãƒƒã‚¯ï¼ˆYYMMDDå½¢å¼ï¼‰
                                    elif date_part.isdigit() and len(date_part) == 6:
                                        year = int("20" + date_part[:2]) if int(date_part[:2]) < 50 else int("19" + date_part[:2])
                                        month = int(date_part[2:4])
                                        day = int(date_part[4:6])
                                        
                                        if 1900 <= year <= 2030 and 1 <= month <= 12 and 1 <= day <= 31:
                                            is_valid_date = True
                                except (ValueError, TypeError, IndexError):
                                    pass
                                
                                if not is_valid_date:
                                    # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯çµ‚äº†åˆ¤å®š
                                    if len(data_lines) > 500:
                                        consecutive_invalid_lines += 1
                                        if consecutive_invalid_lines > 20:  # ã‚ˆã‚Šæ—©ãçµ‚äº†
                                            logger.info(f"ğŸ›‘ ååˆ†ãªãƒ‡ãƒ¼ã‚¿å–å¾—æ¸ˆã¿ã€ç„¡åŠ¹æ—¥ä»˜ã§çµ‚äº†")
                                            break
                                    else:
                                        consecutive_invalid_lines += 1
                                        if consecutive_invalid_lines > max_consecutive_invalid:
                                            logger.info(f"ğŸ›‘ é€£ç¶šç„¡åŠ¹æ—¥ä»˜ãŒ{max_consecutive_invalid}è¡Œç¶šã„ãŸãŸã‚çµ‚äº†")
                                            break
                                    continue
                                
                                # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆã‚ˆã‚Šå³å¯†ã«ï¼‰
                                valid_numeric_count = 0
                                total_numeric_fields = len(expected_columns) - 1  # æ—¥ä»˜ä»¥å¤–
                                
                                for j in range(1, min(len(parts), len(expected_columns))):
                                    try:
                                        value = float(parts[j])
                                        # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒªã‚¿ãƒ¼ãƒ³ã¯é€šå¸¸-100%ï½+100%ã®ç¯„å›²ï¼‰
                                        if -1.0 <= value <= 1.0:  # å°æ•°å½¢å¼
                                            valid_numeric_count += 1
                                        elif -100.0 <= value <= 100.0:  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆå½¢å¼
                                            valid_numeric_count += 1
                                    except (ValueError, TypeError):
                                        pass
                                
                                # æ•°å€¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æœ‰åŠ¹æ€§åˆ¤å®šï¼ˆ70%ä»¥ä¸ŠãŒæœ‰åŠ¹ï¼‰
                                if valid_numeric_count >= total_numeric_fields * 0.7:
                                    data_lines.append(line_stripped)
                                    consecutive_invalid_lines = 0  # æœ‰åŠ¹è¡Œã§ãƒªã‚»ãƒƒãƒˆ
                                    
                                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
                                    if len(data_lines) % 500 == 0:
                                        logger.info(f"ğŸ“ˆ æŠ½å‡ºæ¸ˆã¿: {len(data_lines)}è¡Œ")
                                else:
                                    consecutive_invalid_lines += 1
                                    if consecutive_invalid_lines > max_consecutive_invalid:
                                        logger.info(f"ğŸ›‘ é€£ç¶šæ•°å€¤ç„¡åŠ¹è¡ŒãŒ{max_consecutive_invalid}è¡Œç¶šã„ãŸãŸã‚çµ‚äº†")
                                        break
                            
                            logger.info(f"ğŸ“ˆ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(data_lines)}")
                            
                            # ãƒ‡ãƒ¼ã‚¿é‡ãƒã‚§ãƒƒã‚¯
                            min_required_lines = 50  # æœ€ä½é™å¿…è¦ãªè¡Œæ•°
                            if len(data_lines) < min_required_lines:
                                logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ãŒä¸è¶³: {len(data_lines)}è¡Œ < {min_required_lines}è¡Œ")
                                logger.info("ğŸ” æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®10è¡Œ:")
                                for idx, line in enumerate(data_lines[:10]):
                                    logger.info(f"  {idx+1}: {line}")
                                raise ValueError(f"ååˆ†ãªãƒ‡ãƒ¼ã‚¿è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {len(data_lines)}è¡Œ < {min_required_lines}è¡Œ")
                            
                            # DataFrameã®ä½œæˆã¨æ¤œè¨¼
                            try:
                                logger.info(f"ğŸ“Š DataFrameä½œæˆé–‹å§‹: {len(data_lines)}è¡Œ")
                                data_io = io.StringIO('\n'.join(data_lines))
                                df = pd.read_csv(data_io, header=None, names=expected_columns)
                                
                                logger.info(f"ğŸ“‹ DataFrameä½œæˆå®Œäº†: {len(df)}è¡Œ x {len(df.columns)}åˆ—")
                                
                                # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
                                validation_errors = []
                                
                                # 1. åŸºæœ¬ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                                if len(df) < min_required_lines:
                                    validation_errors.append(f"è¡Œæ•°ä¸è¶³: {len(df)} < {min_required_lines}")
                                
                                if len(df.columns) != len(expected_columns):
                                    validation_errors.append(f"åˆ—æ•°ä¸ä¸€è‡´: {len(df.columns)} != {len(expected_columns)}")
                                
                                # 2. æ—¥ä»˜åˆ—ã®æ¤œè¨¼
                                try:
                                    # æ—¥ä»˜å¤‰æ›ãƒ†ã‚¹ãƒˆ
                                    test_dates = df['Date'].head(10).astype(str)
                                    valid_date_count = 0
                                    for date_str in test_dates:
                                        try:
                                            if len(date_str) == 8 and date_str.isdigit():
                                                year = int(date_str[:4])
                                                if 1900 <= year <= 2030:
                                                    valid_date_count += 1
                                        except:
                                            pass
                                    
                                    if valid_date_count < len(test_dates) * 0.8:
                                        validation_errors.append(f"æ—¥ä»˜å½¢å¼ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹æ—¥ä»˜ {valid_date_count}/{len(test_dates)}")
                                        
                                except Exception as e:
                                    validation_errors.append(f"æ—¥ä»˜åˆ—æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                
                                # 3. æ•°å€¤åˆ—ã®æ¤œè¨¼
                                numeric_columns = [col for col in expected_columns if col != 'Date']
                                for col in numeric_columns:
                                    if col in df.columns:
                                        try:
                                            # æ•°å€¤å¤‰æ›ãƒ†ã‚¹ãƒˆ
                                            numeric_data = pd.to_numeric(df[col], errors='coerce')
                                            valid_rate = (1 - numeric_data.isna().mean())
                                            
                                            if valid_rate < 0.9:
                                                validation_errors.append(f"{col}åˆ—: æ•°å€¤å¤‰æ›ç‡ {valid_rate:.1%}")
                                            
                                            # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
                                            if valid_rate > 0.5:
                                                q1, q99 = numeric_data.quantile([0.01, 0.99])
                                                outlier_rate = ((numeric_data < q1) | (numeric_data > q99)).mean()
                                                if outlier_rate > 0.1:
                                                    logger.warning(f"âš ï¸ {col}åˆ—ã«ç•°å¸¸å€¤ãŒå¤šã„: {outlier_rate:.1%}")
                                        except Exception as e:
                                            validation_errors.append(f"{col}åˆ—æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                
                                # 4. ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚µãƒãƒªãƒ¼
                                logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚µãƒãƒªãƒ¼:")
                                logger.info(f"   - ç·è¡Œæ•°: {len(df):,}")
                                logger.info(f"   - åˆ—æ•°: {len(df.columns)}")
                                logger.info(f"   - æœŸé–“: {df['Date'].iloc[0]} ï½ {df['Date'].iloc[-1]}")
                                
                                # å„åˆ—ã®çµ±è¨ˆ
                                for col in numeric_columns:
                                    if col in df.columns:
                                        try:
                                            col_data = pd.to_numeric(df[col], errors='coerce')
                                            if not col_data.isna().all():
                                                mean_val = col_data.mean()
                                                std_val = col_data.std()
                                                logger.info(f"   - {col}: å¹³å‡={mean_val:.4f}, æ¨™æº–åå·®={std_val:.4f}")
                                        except:
                                            logger.warning(f"   - {col}: çµ±è¨ˆè¨ˆç®—ä¸å¯")
                                
                                # æ¤œè¨¼çµæœåˆ¤å®š
                                if validation_errors:
                                    logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿å“è³ªã®è­¦å‘Š ({len(validation_errors)}ä»¶):")
                                    for error in validation_errors:
                                        logger.warning(f"   - {error}")
                                    
                                    # è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
                                    critical_errors = [e for e in validation_errors if any(keyword in e.lower() 
                                                     for keyword in ['è¡Œæ•°ä¸è¶³', 'åˆ—æ•°ä¸ä¸€è‡´', 'æ—¥ä»˜å½¢å¼ã‚¨ãƒ©ãƒ¼'])]
                                    
                                    if critical_errors:
                                        logger.error(f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {critical_errors}")
                                        raise ValueError(f"ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¨ãƒ©ãƒ¼: {'; '.join(critical_errors)}")
                                    else:
                                        logger.info("âœ… è­¦å‘Šã¯ã‚ã‚Šã¾ã™ãŒã€ä½¿ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã§ã™")
                                else:
                                    logger.info("âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼: å…¨ãƒã‚§ãƒƒã‚¯é€šé")
                                
                                logger.info(f"âœ… {data_type}ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(df)}è¡Œ x {len(df.columns)}åˆ—")
                                return df
                                
                            except Exception as e:
                                logger.error(f"âŒ DataFrameä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                                logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°ç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
                                for idx, line in enumerate(data_lines[:5]):
                                    logger.info(f"  {idx+1}: {line}")
                                raise ValueError(f"DataFrameä½œæˆã«å¤±æ•—: {str(e)}")
                            
                except Exception as e:
                    logger.warning(f"âŒ {data_type}ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— (è©¦è¡Œ {retry+1}/3): {str(e)}")
                    if retry < 2:  # æœ€å¾Œã®è©¦è¡Œã§ãªã‘ã‚Œã°å¾…æ©Ÿ
                        wait_time = (retry + 1) * 2
                        logger.info(f"â±ï¸ {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                        time.sleep(wait_time)
                    continue
        
        raise Exception(f"ã™ã¹ã¦ã®{data_type}ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸ")
    
    try:
        # 1. Fama-French 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        ff5_columns = ['Date', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']
        ff5_df = robust_download_and_parse(ff5_urls, "5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼", ff5_columns)
        
        # 2. Momentumãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        mom_columns = ['Date', 'Mom']
        mom_df = robust_download_and_parse(mom_urls, "Momentum", mom_columns)
        
        # 3. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨çµåˆ
        def parse_ff_date(date_str):
            """Kenneth Frenchã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆYYYYMMDDï¼‰ã‚’å …ç‰¢ã«ãƒ‘ãƒ¼ã‚¹"""
            try:
                date_str = str(date_str).strip()
                if len(date_str) == 8 and date_str.isdigit():
                    year = int(date_str[:4])
                    month = int(date_str[4:6])
                    day = int(date_str[6:8])
                    
                    # æ—¥ä»˜ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                    if 1900 <= year <= 2030 and 1 <= month <= 12 and 1 <= day <= 31:
                        return pd.to_datetime(f"{year}-{month:02d}-{day:02d}")
                
                return pd.NaT
            except (ValueError, TypeError):
                return pd.NaT
        
        logger.info("ğŸ“… æ—¥ä»˜å¤‰æ›å‡¦ç†ä¸­...")
        
        # æ—¥ä»˜å¤‰æ›
        ff5_df['Date'] = ff5_df['Date'].apply(parse_ff_date)
        mom_df['Date'] = mom_df['Date'].apply(parse_ff_date)
        
        # ç„¡åŠ¹ãªæ—¥ä»˜ã‚’å‰Šé™¤
        ff5_df = ff5_df.dropna(subset=['Date'])
        mom_df = mom_df.dropna(subset=['Date'])
        
        logger.info(f"ğŸ“Š æ—¥ä»˜å¤‰æ›å¾Œ: 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ {len(ff5_df)}è¡Œ, Momentum {len(mom_df)}è¡Œ")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ—¥ä»˜ã«è¨­å®š
        ff5_df.set_index('Date', inplace=True)
        mom_df.set_index('Date', inplace=True)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆå†…éƒ¨çµåˆã§å…±é€šã®æ—¥ä»˜ã®ã¿ï¼‰
        logger.info("ğŸ”— ãƒ‡ãƒ¼ã‚¿çµåˆä¸­...")
        factors = ff5_df.join(mom_df, how='inner')
        
        if factors.empty:
            raise ValueError("5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¨Momentumãƒ‡ãƒ¼ã‚¿ã®çµåˆã«å¤±æ•—")
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿çµåˆæˆåŠŸ: {len(factors)}è¡Œ")
        
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‹ã‚‰å°æ•°ã«å¤‰æ›
        factors = factors.div(100)
        
        # æŒ‡å®šæœŸé–“ã§ãƒ•ã‚£ãƒ«ã‚¿
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date)
        factors = factors[(factors.index >= start_dt) & (factors.index <= end_dt)]
        
        # æ•°å€¤å‹ã¸ã®å¤‰æ›ã¨ç•°å¸¸å€¤é™¤å»
        for col in factors.columns:
            factors[col] = pd.to_numeric(factors[col], errors='coerce')
            
            # ç•°å¸¸å€¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒªã‚¿ãƒ¼ãƒ³ã¯é€šå¸¸-50%ï½+50%ã®ç¯„å›²ï¼‰
            q1 = factors[col].quantile(0.01)
            q99 = factors[col].quantile(0.99)
            factors.loc[(factors[col] < q1) | (factors[col] > q99), col] = np.nan
        
        # æ¬ æå€¤ã‚’å‰Šé™¤
        factors = factors.dropna()
        
        if factors.empty:
            raise ValueError("æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿å¾Œã«ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚Šã¾ã—ãŸ")
        
        logger.info(f"ğŸ¯ Kenneth Frenchå…¬å¼ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†!")
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼:")
        logger.info(f"   - æœŸé–“: {factors.index.min().strftime('%Y-%m-%d')} ï½ {factors.index.max().strftime('%Y-%m-%d')}")
        logger.info(f"   - æ—¥æ•°: {len(factors)}æ—¥")
        logger.info(f"   - ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: {list(factors.columns)}")
        
        return factors
        
    except Exception as e:
        logger.error(f"âŒ Kenneth Frenchå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã®ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æœ€çµ‚ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return pd.DataFrame()


def download_fred_factor_data(start_date: str, end_date: str) -> pd.DataFrame:
    """
    FREDï¼ˆFederal Reserve Economic Dataï¼‰ã‹ã‚‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    å®Œå…¨ãªFama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã§ã¯ãªã„ãŒã€ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—å¯èƒ½
    
    Args:
        start_date: é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
        end_date: çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
    
    Returns:
        pd.DataFrame: åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼é–¢é€£ãƒ‡ãƒ¼ã‚¿
    """
    try:
        import requests
        import json
        
        logger.info("FRED APIã‹ã‚‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼é–¢é€£ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œ...")
        
        # FREDã‹ã‚‰ã¯ãƒªã‚¹ã‚¯ãƒ•ãƒªãƒ¼ãƒ¬ãƒ¼ãƒˆãªã©ã®åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—å¯èƒ½
        # å®Œå…¨ãªFama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¯å–å¾—ã§ããªã„ãŸã‚ã€åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®ã¿
        
        base_url = "https://api.stlouisfed.org/fred/series/observations"
        
        # åˆ©ç”¨å¯èƒ½ãªã‚·ãƒªãƒ¼ã‚º
        series_list = {
            'DGS3MO': 'RiskFree_3M',     # 3ãƒ¶æœˆå›½å‚µåˆ©å›ã‚Š
            'DGS10': 'RiskFree_10Y',     # 10å¹´å›½å‚µåˆ©å›ã‚Š
            'FEDFUNDS': 'FedFunds',      # ãƒ•ã‚§ãƒ‡ãƒ©ãƒ«ãƒ•ã‚¡ãƒ³ãƒ‰é‡‘åˆ©
        }
        
        factors_data = {}
        
        for series_id, factor_name in series_list.items():
            try:
                params = {
                    'series_id': series_id,
                    'api_key': 'YOUR_FRED_API_KEY',  # å®Ÿéš›ã®APIã‚­ãƒ¼ãŒå¿…è¦
                    'file_type': 'json',
                    'observation_start': start_date,
                    'observation_end': end_date,
                    'frequency': 'd',  # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿
                    'aggregation_method': 'avg'
                }
                
                response = requests.get(base_url, params=params, timeout=15)
                response.raise_for_status()
                
                data = response.json()
                
                if 'observations' in data:
                    dates = []
                    values = []
                    
                    for obs in data['observations']:
                        if obs['value'] != '.':  # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿
                            dates.append(pd.to_datetime(obs['date']))
                            values.append(float(obs['value']) / 100)  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆã‹ã‚‰å°æ•°ã«
                    
                    if dates and values:
                        series = pd.Series(values, index=dates, name=factor_name)
                        factors_data[factor_name] = series
                        logger.info(f"FRED {series_id} ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(series)}æ—¥åˆ†")
                
            except Exception as e:
                logger.warning(f"FRED {series_id} ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {str(e)}")
        
        if factors_data:
            # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            factors_df = pd.DataFrame(factors_data)
            factors_df = factors_df.dropna()
            
            # åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’æ§‹ç¯‰ï¼ˆé™å®šçš„ï¼‰
            if 'RiskFree_3M' in factors_df.columns:
                factors_df['RF'] = factors_df['RiskFree_3M'] / 252  # å¹´ç‡ã‹ã‚‰æ—¥æ¬¡ã«å¤‰æ›
            elif 'FedFunds' in factors_df.columns:
                factors_df['RF'] = factors_df['FedFunds'] / 252
            else:
                factors_df['RF'] = 0.00008  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            logger.info(f"FRED ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(factors_df)}æ—¥åˆ†")
            return factors_df
        else:
            logger.warning("FRED ã‹ã‚‰æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame()
            
    except Exception as e:
        logger.error(f"FRED API ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return pd.DataFrame()


def get_fama_french_factors(start_date: str = None, end_date: str = None) -> pd.DataFrame:
    """
    Fama-French 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ + Momentumãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’å–å¾—
    è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿æºã‚’è©¦è¡Œã—ã€å¤±æ•—æ™‚ã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    
    Args:
        start_date: é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
        end_date: çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
    
    Returns:
        pd.DataFrame: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆMkt-RF, SMB, HML, RMW, CMA, Mom, RFï¼‰
    """
    try:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§éå»3å¹´é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=3*365)).strftime('%Y-%m-%d')
        
        logger.info(f"Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¾—é–‹å§‹: {start_date} to {end_date}")
        
        # 1. Kenneth Frenchå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        try:
            logger.info("ğŸ¯ Kenneth Frenchå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥Fama-Frenchãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œ...")
            factors = download_fama_french_direct(start_date, end_date)
            if not factors.empty and len(factors) > 10:
                logger.info(f"âœ… å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰å®Ÿéš›ã®Fama-Frenchãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(factors)}æ—¥åˆ†")
                logger.info(f"åˆ©ç”¨å¯èƒ½ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: {list(factors.columns)}")
                logger.info(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {factors.index.min()} ï½ {factors.index.max()}")
                
                # Streamlitç”¨ã®æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                try:
                    import streamlit as st
                    with st.expander("ğŸ¯ å…¬å¼Fama-Frenchãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ä¸­", expanded=False):
                        st.success("""
                        **Kenneth Frenchå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰å®Ÿéš›ã®Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ**
                        
                        - ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: Dartmouth Tuck School of Business
                        - ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: 5-Factor + Momentum (Mkt-RF, SMB, HML, RMW, CMA, Mom, RF)
                        - ã“ã‚Œã¯å®Ÿéš›ã®å­¦è¡“ç ”ç©¶ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™
                        - åˆ†æçµæœã¯å®Œå…¨ã«ä¿¡é ¼ã§ãã¾ã™
                        """)
                except:
                    pass
                
                return factors
        except Exception as e:
            logger.warning(f"å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã®ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {str(e)}")
        
        # 2. pandas_datareaderã‚’è©¦è¡Œï¼ˆè¤‡æ•°å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
        for attempt in range(3):  # æœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤
            try:
                import pandas_datareader.data as web
                logger.info(f"pandas_datareaderã§Fama-Frenchãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œ... (è©¦è¡Œ {attempt + 1}/3)")
                
                # ã‚ˆã‚Šé•·ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ãƒªãƒˆãƒ©ã‚¤
                timeout = 20 + (attempt * 10)  # 20, 30, 40ç§’
                
                # Fama-French 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’å–å¾—
                logger.info("F-F 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
                ff5 = web.DataReader('F-F_Research_Data_5_Factors_2x3_daily', 'famafrench', 
                                   start=start_date, end=end_date, timeout=timeout)[0]
                
                # Momentumãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’å–å¾—
                logger.info("Momentumãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
                mom = web.DataReader('F-F_Momentum_Factor_daily', 'famafrench', 
                                   start=start_date, end=end_date, timeout=timeout)[0]
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã€ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‹ã‚‰å°æ•°ã«å¤‰æ›
                factors = ff5.join(mom, how='inner').div(100)  # %â†’decimal
                
                if not factors.empty and len(factors) > 10:  # æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯
                    logger.info(f"âœ… å®Ÿéš›ã®Fama-Frenchãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(factors)}æ—¥åˆ†")
                    logger.info(f"åˆ©ç”¨å¯èƒ½ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: {list(factors.columns)}")
                    logger.info(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {factors.index.min()} ï½ {factors.index.max()}")
                    return factors
                else:
                    raise ValueError("å–å¾—ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†")
                
            except ImportError:
                logger.warning("pandas_datareaderãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»£æ›¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œã—ã¾ã™ã€‚")
                break  # ImportErrorã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤ä¸è¦
            except Exception as e:
                logger.warning(f"pandas_datareaderã§ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•— (è©¦è¡Œ {attempt + 1}/3): {str(e)}")
                if attempt == 2:  # æœ€å¾Œã®è©¦è¡Œ
                    logger.warning("å…¨ã¦ã®è©¦è¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ä»£æ›¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œã—ã¾ã™ã€‚")
                else:
                    import time
                    time.sleep(2)  # 2ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤
        
        # 3. yfinanceã‚’ä½¿ã£ãŸä»£æ›¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œï¼ˆå®Ÿéš›ã®Fama-Frenchã«è¿‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰
        try:
            logger.info("ğŸ“ˆ yfinanceã§å®Ÿéš›ã®ETFãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼æ§‹ç¯‰ã‚’è©¦è¡Œ...")
            factors = get_proxy_factor_data(start_date, end_date)
            if not factors.empty and len(factors) > 10:
                logger.info(f"âœ… yfinanceã§å®Ÿéš›ã®ETFãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼æ§‹ç¯‰å®Œäº†: {len(factors)}æ—¥åˆ†")
                logger.info(f"åˆ©ç”¨å¯èƒ½ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: {list(factors.columns)}")
                
                # Streamlitç”¨ã®æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                try:
                    import streamlit as st
                    with st.expander("âœ… ä»£æ›¿ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ä¸­", expanded=False):
                        st.success("""
                        **å®Ÿéš›ã®ETFãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸ**
                        
                        å®Ÿéš›ã®pandas_datareaderãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€yfinanceã‹ã‚‰å–å¾—ã—ãŸ
                        å®Ÿéš›ã®ETFãƒ‡ãƒ¼ã‚¿ï¼ˆSPYã€IWMã€VTVã€VUGç­‰ï¼‰ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚
                        
                        ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€å®Ÿéš›ã®Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã«è¿‘ã„åˆ†æãŒå¯èƒ½ã§ã™ã€‚
                        """)
                except:
                    pass
                
                return factors
            else:
                raise ValueError("ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†")
        except Exception as e:
            logger.warning(f"yfinanceã§ã®ä»£æ›¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {str(e)}")
            logger.info("çµ±è¨ˆçš„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        # 4. æœ€çµ‚æ‰‹æ®µï¼šã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        logger.warning("å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ãŸãŸã‚ã€çµ±è¨ˆçš„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
        return create_sample_factor_data(start_date, end_date)
        
    except Exception as e:
        logger.error(f"Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¾—ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # æœ€çµ‚çš„ã«ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        return create_sample_factor_data(start_date, end_date)


def get_proxy_factor_data(start_date: str, end_date: str) -> pd.DataFrame:
    """
    yfinanceã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®ä»£ç†æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Args:
        start_date: é–‹å§‹æ—¥
        end_date: çµ‚äº†æ—¥
    
    Returns:
        pd.DataFrame: ä»£ç†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
    """
    try:
        logger.info(f"ä»£ç†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {start_date} to {end_date}")
        
        # å®Ÿéš›ã®Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã«æœ€ã‚‚è¿‘ã„ä»£ç†æŒ‡æ¨™ã‚’é¸æŠ
        proxy_tickers = {
            # Market factor
            'SPY': 'Market',          # S&P500 (Mkt-RFä»£ç†)
            'VTI': 'Market_Broad',    # Total Stock Market 
            
            # Size factor (SMB)
            'IWM': 'Small_Cap',       # Russell 2000 Small Cap
            'VB': 'Small_Cap_Alt',    # Vanguard Small Cap
            'IWB': 'Large_Cap',       # Russell 1000 Large Cap
            
            # Value vs Growth (HML)
            'VTV': 'Value',           # Vanguard Value ETF
            'VUG': 'Growth',          # Vanguard Growth ETF
            'IWD': 'Value_Alt',       # iShares Russell 1000 Value
            'IWF': 'Growth_Alt',      # iShares Russell 1000 Growth
            
            # Profitability/Quality (RMWä»£ç†)
            'QUAL': 'Quality',        # iShares MSCI Quality Factor
            'VYM': 'HighDiv',         # High Dividend (åç›Šæ€§ä»£ç†)
            'NOBL': 'Dividend_Aris',  # Dividend Aristocrats
            
            # Investment (CMAä»£ç†)
            'VMOT': 'Conservative',   # Conservative allocation
            'VEA': 'International',   # International developed
            
            # Momentum (Mom)
            'MTUM': 'Momentum',       # iShares Momentum Factor
            'PDP': 'Momentum_Alt',    # Dividend momentum
            
            # Risk-free rate
            '^TNX': 'RiskFree'        # 10å¹´å‚µåˆ©å›ã‚Š
        }
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        import yfinance as yf
        
        price_data = {}
        successful_tickers = []
        
        for ticker, name in proxy_tickers.items():
            try:
                logger.info(f"å–å¾—ä¸­: {name}({ticker})")
                data = yf.download(ticker, start=start_date, end=end_date, progress=False, timeout=30)
                if not data.empty and 'Adj Close' in data.columns:
                    price_data[name] = data['Adj Close'].dropna()
                    successful_tickers.append(f"{name}({ticker})")
                    logger.info(f"{name}({ticker}) ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(data)}æ—¥")
                else:
                    logger.warning(f"{name}({ticker}) ãƒ‡ãƒ¼ã‚¿ãŒç©ºã¾ãŸã¯ç„¡åŠ¹")
            except Exception as e:
                logger.warning(f"{name}({ticker}) å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        logger.info(f"æˆåŠŸã—ãŸå–å¾—: {len(successful_tickers)} / {len(proxy_tickers)} ãƒ†ã‚£ãƒƒã‚«ãƒ¼")
        logger.info(f"æˆåŠŸãƒ†ã‚£ãƒƒã‚«ãƒ¼: {', '.join(successful_tickers)}")
        
        if len(price_data) < 3:  # æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ãŒæƒã‚ãªã„å ´åˆ
            logger.warning("ä»£ç†æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã™")
            return pd.DataFrame()
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—
        returns_data = {}
        for name, prices in price_data.items():
            if len(prices) > 1:
                returns = prices.pct_change().dropna()
                returns_data[name] = returns
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«çµåˆ
        returns_df = pd.DataFrame(returns_data).dropna()
        
        if returns_df.empty:
            return pd.DataFrame()
        
        # Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’å®Ÿéš›ã®æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯ã«è¿‘ã„å½¢ã§è¨ˆç®—
        factors = pd.DataFrame(index=returns_df.index)
        
        # ãƒªã‚¹ã‚¯ãƒ•ãƒªãƒ¼ãƒ¬ãƒ¼ãƒˆï¼ˆ10å¹´å‚µåˆ©å›ã‚Šã‹ã‚‰æ¨å®šï¼‰
        if 'RiskFree' in returns_df.columns:
            # 10å¹´å‚µåˆ©å›ã‚Šã®æ—¥æ¬¡å¤‰åŒ–ã‚’å¹´ç‡æ›ç®—ã—ã¦ãƒªã‚¹ã‚¯ãƒ•ãƒªãƒ¼ãƒ¬ãƒ¼ãƒˆã¨ã—ã¦ä½¿ç”¨
            factors['RF'] = returns_df['RiskFree'].abs() / 252
        else:
            factors['RF'] = 0.00008  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå¹´ç‡2%ã®æ—¥æ¬¡ç›¸å½“ï¼‰
        
        # å¸‚å ´ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ï¼ˆMkt-RFï¼‰- æœ€ã‚‚é‡è¦ãªãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        market_return = None
        if 'Market' in returns_df.columns:
            market_return = returns_df['Market']
        elif 'Market_Broad' in returns_df.columns:
            market_return = returns_df['Market_Broad']
        
        if market_return is not None:
            factors['Mkt-RF'] = market_return - factors['RF']
            logger.info("âœ… å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Mkt-RFãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¨ˆç®—")
        else:
            factors['Mkt-RF'] = np.random.normal(0.0008, 0.012, len(factors))
            logger.warning("âš ï¸ å¸‚å ´ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚çµ±è¨ˆçš„ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        
        # Small Minus Big (SMB) - ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        small_return = None
        large_return = None
        
        # å°å‹æ ªãƒªã‚¿ãƒ¼ãƒ³
        if 'Small_Cap' in returns_df.columns:
            small_return = returns_df['Small_Cap']
        elif 'Small_Cap_Alt' in returns_df.columns:
            small_return = returns_df['Small_Cap_Alt']
        
        # å¤§å‹æ ªãƒªã‚¿ãƒ¼ãƒ³
        if 'Large_Cap' in returns_df.columns:
            large_return = returns_df['Large_Cap']
        elif market_return is not None:
            large_return = market_return  # å¸‚å ´ã‚’å¤§å‹æ ªã®ä»£ç†ã¨ã—ã¦ä½¿ç”¨
        
        if small_return is not None and large_return is not None:
            factors['SMB'] = small_return - large_return
            logger.info("âœ… å®Ÿéš›ã®å°å‹ãƒ»å¤§å‹æ ªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰SMBãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¨ˆç®—")
        else:
            factors['SMB'] = np.random.normal(0.0001, 0.008, len(factors))
            logger.warning("âš ï¸ ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # High Minus Low (HML) - ãƒãƒªãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        value_return = None
        growth_return = None
        
        if 'Value' in returns_df.columns:
            value_return = returns_df['Value']
        elif 'Value_Alt' in returns_df.columns:
            value_return = returns_df['Value_Alt']
        
        if 'Growth' in returns_df.columns:
            growth_return = returns_df['Growth']
        elif 'Growth_Alt' in returns_df.columns:
            growth_return = returns_df['Growth_Alt']
        
        if value_return is not None and growth_return is not None:
            factors['HML'] = value_return - growth_return
            logger.info("âœ… å®Ÿéš›ã®ãƒãƒªãƒ¥ãƒ¼ãƒ»ã‚°ãƒ­ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰HMLãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¨ˆç®—")
        else:
            factors['HML'] = np.random.normal(0.0002, 0.007, len(factors))
            logger.warning("âš ï¸ ãƒãƒªãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # Robust Minus Weak (RMW) - åç›Šæ€§ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        quality_return = None
        if 'Quality' in returns_df.columns:
            quality_return = returns_df['Quality']
        elif 'HighDiv' in returns_df.columns:
            quality_return = returns_df['HighDiv']
        elif 'Dividend_Aris' in returns_df.columns:
            quality_return = returns_df['Dividend_Aris']
        
        if quality_return is not None and market_return is not None:
            factors['RMW'] = quality_return - market_return
            logger.info("âœ… å®Ÿéš›ã®å“è³ªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰RMWãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¨ˆç®—")
        else:
            factors['RMW'] = np.random.normal(0.0001, 0.005, len(factors))
            logger.warning("âš ï¸ åç›Šæ€§ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # Conservative Minus Aggressive (CMA) - æŠ•è³‡ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        conservative_return = None
        if 'Conservative' in returns_df.columns:
            conservative_return = returns_df['Conservative']
        elif 'International' in returns_df.columns:
            conservative_return = returns_df['International']
        
        if conservative_return is not None and market_return is not None:
            factors['CMA'] = (conservative_return - market_return) * 0.5
            logger.info("âœ… å®Ÿéš›ã®æŠ•è³‡ã‚¹ã‚¿ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰CMAãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¨ˆç®—")
        elif quality_return is not None:
            factors['CMA'] = -quality_return * 0.3  # å“è³ªã®é€†ã¨ã—ã¦è¿‘ä¼¼
        else:
            factors['CMA'] = np.random.normal(-0.0001, 0.006, len(factors))
            logger.warning("âš ï¸ æŠ•è³‡ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # Momentum (Mom) - ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        momentum_return = None
        if 'Momentum' in returns_df.columns:
            momentum_return = returns_df['Momentum']
        elif 'Momentum_Alt' in returns_df.columns:
            momentum_return = returns_df['Momentum_Alt']
        
        if momentum_return is not None and market_return is not None:
            factors['Mom'] = momentum_return - market_return
            logger.info("âœ… å®Ÿéš›ã®ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Momãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¨ˆç®—")
        elif market_return is not None:
            # å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç§»å‹•å¹³å‡ã‚’ä½¿ã£ã¦ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã‚’è¨ˆç®—
            momentum_window = min(21, len(market_return) // 3)  # ç´„1ãƒ¶æœˆ
            if momentum_window > 5:
                # éå»ãƒªã‚¿ãƒ¼ãƒ³ã®ç§»å‹•å¹³å‡ - ç¾åœ¨ã®ãƒªã‚¿ãƒ¼ãƒ³
                past_returns = market_return.rolling(window=momentum_window).mean().shift(1)
                factors['Mom'] = (past_returns - market_return).fillna(0) * 2
                logger.info("âœ… å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’æ¨å®šè¨ˆç®—")
            else:
                factors['Mom'] = np.random.normal(0.0003, 0.009, len(factors))
        else:
            factors['Mom'] = np.random.normal(0.0003, 0.009, len(factors))
            logger.warning("âš ï¸ ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        logger.info(f"ä»£ç†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰å®Œäº†: {len(factors)}æ—¥åˆ†")
        return factors
        
    except Exception as e:
        logger.error(f"ä»£ç†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return pd.DataFrame()


def create_sample_factor_data(start_date: str, end_date: str) -> pd.DataFrame:
    """
    çµ±è¨ˆçš„ã«ç¾å®Ÿçš„ãªã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆï¼‰
    
    Args:
        start_date: é–‹å§‹æ—¥
        end_date: çµ‚äº†æ—¥
    
    Returns:
        pd.DataFrame: ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
    """
    try:
        logger.info(f"çµ±è¨ˆçš„ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹: {start_date} to {end_date}")
        
        # æ—¥ä»˜ç¯„å›²ã‚’ç”Ÿæˆ
        date_range = pd.date_range(start=start_date, end=end_date, freq='D')
        # å–¶æ¥­æ—¥ã®ã¿ã‚’æŠ½å‡º
        business_days = date_range[date_range.weekday < 5]
        
        if len(business_days) == 0:
            logger.error("æœ‰åŠ¹ãªå–¶æ¥­æ—¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return pd.DataFrame()
        
        # å„ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®å…¸å‹çš„ãªçµ±è¨ˆç‰¹æ€§ã«åŸºã¥ã„ã¦ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        # å®Ÿéš›ã®Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®æ­´å²çš„çµ±è¨ˆå€¤ã«åŸºã¥ã
        np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        n_days = len(business_days)
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªç›¸é–¢ã‚’æŒã¤ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’ç”Ÿæˆ
        # ã¾ãšç‹¬ç«‹æˆåˆ†ã‚’ç”Ÿæˆ
        independent_factors = np.random.multivariate_normal(
            mean=[0.0008, 0.0001, 0.0002, 0.0001, -0.0001, 0.0003],
            cov=np.array([
                [0.000144, 0.00002, 0.00001, 0.00001, -0.00001, 0.00002],  # Mkt-RF
                [0.00002, 0.000064, -0.00001, 0.000005, 0.000002, 0.00001],  # SMB
                [0.00001, -0.00001, 0.000049, 0.000008, 0.000004, -0.00001],  # HML
                [0.00001, 0.000005, 0.000008, 0.000025, -0.000003, 0.000003],  # RMW
                [-0.00001, 0.000002, 0.000004, -0.000003, 0.000036, -0.000002],  # CMA
                [0.00002, 0.00001, -0.00001, 0.000003, -0.000002, 0.000081]   # Mom
            ]),
            size=n_days
        )
        
        sample_data = pd.DataFrame({
            'Mkt-RF': independent_factors[:, 0],    # å¸‚å ´ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
            'SMB': independent_factors[:, 1],       # å°å‹æ ªãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
            'HML': independent_factors[:, 2],       # ãƒãƒªãƒ¥ãƒ¼ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
            'RMW': independent_factors[:, 3],       # åç›Šæ€§ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
            'CMA': independent_factors[:, 4],       # æŠ•è³‡ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
            'Mom': independent_factors[:, 5],       # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
            'RF': np.maximum(                       # ãƒªã‚¹ã‚¯ãƒ•ãƒªãƒ¼ãƒ¬ãƒ¼ãƒˆï¼ˆè² ã«ãªã‚‰ãªã„ã‚ˆã†ã«ï¼‰
                np.random.normal(0.00008, 0.00003, n_days),  # å¹´ç‡2%ç¨‹åº¦ã®æ—¥æ¬¡
                0.00001  # æœ€å°å€¤
            )
        }, index=business_days)
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        if sample_data.isnull().any().any():
            logger.warning("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã«NaNå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
            sample_data = sample_data.fillna(0)
        
        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(sample_data)}æ—¥åˆ†")
        for col in sample_data.columns:
            mean_val = sample_data[col].mean()
            std_val = sample_data[col].std()
            logger.info(f"{col}: å¹³å‡={mean_val:.6f}, æ¨™æº–åå·®={std_val:.6f}")
        
        # Streamlitç”¨ã®è­¦å‘Šè¡¨ç¤º
        try:
            import streamlit as st
            with st.expander("âš ï¸ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦", expanded=True):
                st.warning("""
                **å®Ÿéš›ã®Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ**
                
                ç¾åœ¨ã€çµ±è¨ˆçš„ã«ç¾å®Ÿçš„ãªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
                
                **ä½¿ç”¨ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´:**
                - å®Ÿéš›ã®Fama-French 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ + ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã®çµ±è¨ˆç‰¹æ€§ã«åŸºã¥ã„ã¦ç”Ÿæˆ
                - ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼é–“ã®ç›¸é–¢é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸæ§‹ç¯‰
                - æ­´å²çš„ãªå¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ã¨å¤‰å‹•æ€§ã‚’å†ç¾
                
                **åˆ†æçµæœã®è§£é‡ˆ:**
                - ãƒ™ãƒ¼ã‚¿å€¤ã®å¤§å°é–¢ä¿‚ã¯å‚è€ƒã«ãªã‚Šã¾ã™
                - çµ¶å¯¾å€¤ã¯å‚è€ƒå€¤ã¨ã—ã¦è§£é‡ˆã—ã¦ãã ã•ã„
                - ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®ã‚¹ã‚¿ã‚¤ãƒ«åˆ†æã«ã¯ååˆ†æ´»ç”¨ã§ãã¾ã™
                
                **å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã«ã¯:**
                1. `pip install pandas-datareader` ã‚’å®Ÿè¡Œ
                2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª
                3. ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„
                """)
                
                st.info("""
                **Fama-French 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ + ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã¨ã¯:**
                - **Mkt-RF**: å¸‚å ´ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ï¼ˆå¸‚å ´ãƒªã‚¿ãƒ¼ãƒ³ - ãƒªã‚¹ã‚¯ãƒ•ãƒªãƒ¼ãƒ¬ãƒ¼ãƒˆï¼‰
                - **SMB**: ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆSmall Minus Bigï¼‰
                - **HML**: ãƒãƒªãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆHigh Minus Lowï¼‰
                - **RMW**: åç›Šæ€§ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆRobust Minus Weakï¼‰
                - **CMA**: æŠ•è³‡ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆConservative Minus Aggressiveï¼‰
                - **Mom**: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
                """)
        except:
            pass
        
        return sample_data
        
    except Exception as e:
        logger.error(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        # æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        try:
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            business_days = date_range[date_range.weekday < 5]
            
            if len(business_days) > 0:
                minimal_data = pd.DataFrame({
                    'Mkt-RF': [0.0008] * len(business_days),
                    'SMB': [0.0001] * len(business_days),
                    'HML': [0.0002] * len(business_days),
                    'RMW': [0.0001] * len(business_days),
                    'CMA': [-0.0001] * len(business_days),
                    'Mom': [0.0003] * len(business_days),
                    'RF': [0.00008] * len(business_days)
                }, index=business_days)
                
                logger.info("æœ€ä½é™ã®ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
                return minimal_data
        except:
            pass
        
        return pd.DataFrame()


def calculate_portfolio_returns_robust(
    pnl_df: pd.DataFrame,
    period: str = '1y'
) -> pd.Series:
    """
    ãƒ­ãƒã‚¹ãƒˆãªãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ—¥æ¬¡ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
    ãƒªã‚¹ã‚¯åˆ†æã¨åŒã˜æ–¹å¼ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—
    
    Args:
        pnl_df: æç›Šè¨ˆç®—æ¸ˆã¿DataFrameï¼ˆticker, shares, current_value_jpyåˆ—ã‚’å«ã‚€ï¼‰
        period: ãƒ‡ãƒ¼ã‚¿å–å¾—æœŸé–“ï¼ˆãƒªã‚¹ã‚¯åˆ†æã‚¿ãƒ–ã§é¸æŠã•ã‚ŒãŸæœŸé–“ï¼‰
    
    Returns:
        pd.Series: ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ—¥æ¬¡ãƒªã‚¿ãƒ¼ãƒ³
    """
    try:
        logger.info(f"ãƒ­ãƒã‚¹ãƒˆãªãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—é–‹å§‹: æœŸé–“={period}")
        
        # å¿…è¦ãªåˆ—ã®ç¢ºèª
        required_cols = ['ticker', 'current_value_jpy']
        missing_cols = [col for col in required_cols if col not in pnl_df.columns]
        if missing_cols:
            logger.error(f"å¿…è¦ãªåˆ—ãŒä¸è¶³: {missing_cols}, åˆ©ç”¨å¯èƒ½: {pnl_df.columns.tolist()}")
            return pd.Series()
        
        tickers = pnl_df['ticker'].tolist()
        logger.info(f"å¯¾è±¡éŠ˜æŸ„æ•°: {len(tickers)}, ãƒ†ã‚£ãƒƒã‚«ãƒ¼: {tickers}")
        
        # ãƒªã‚¹ã‚¯åˆ†æã¨åŒã˜æ–¹å¼ã§éå»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        from modules.price_fetcher import get_historical_data
        from utils.helpers import calculate_returns
        
        logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {tickers}, æœŸé–“={period}")
        historical_data = get_historical_data(tickers, period=period)
        
        if historical_data.empty:
            logger.error("éå»ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—")
            return pd.Series()
        
        logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {historical_data.shape}")
        
        # æ—¥æ¬¡ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—ï¼ˆãƒªã‚¹ã‚¯åˆ†æã¨åŒã˜æ–¹å¼ï¼‰
        returns_df = pd.DataFrame()
        for ticker in tickers:
            if ticker in historical_data.columns:
                returns = calculate_returns(historical_data[ticker])
                if not returns.empty:
                    returns_df[ticker] = returns
        
        if returns_df.empty:
            logger.error("ãƒªã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®—ã«å¤±æ•—")
            return pd.Series()
        
        logger.info(f"æ—¥æ¬¡ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—å®Œäº†: {returns_df.shape}")
        
        # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªé‡ã¿ã‚’è¨ˆç®—ï¼ˆãƒªã‚¹ã‚¯åˆ†æã¨åŒã˜æ–¹å¼ï¼‰
        total_value = pnl_df['current_value_jpy'].sum()
        if total_value <= 0:
            logger.error("ç·æ™‚ä¾¡ç·é¡ãŒ0ä»¥ä¸‹ã§ã™")
            return pd.Series()
        
        # ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã‚‹éŠ˜æŸ„ã®ã¿ã§ã‚¦ã‚§ã‚¤ãƒˆã‚’è¨ˆç®—
        valid_tickers = [ticker for ticker in tickers if ticker in returns_df.columns]
        valid_pnl = pnl_df[pnl_df['ticker'].isin(valid_tickers)]
        
        if len(valid_tickers) == 0:
            logger.error("æœ‰åŠ¹ãªéŠ˜æŸ„ãŒ0ä»¶ã§ã™")
            return pd.Series()
        
        logger.info(f"æœ‰åŠ¹éŠ˜æŸ„æ•°: {len(valid_tickers)}/{len(tickers)}")
        
        # æœ‰åŠ¹ãªéŠ˜æŸ„ã®ã‚¦ã‚§ã‚¤ãƒˆã‚’å†è¨ˆç®—
        valid_total_value = valid_pnl['current_value_jpy'].sum()
        if valid_total_value <= 0:
            logger.error("æœ‰åŠ¹éŠ˜æŸ„ã®ç·æ™‚ä¾¡ç·é¡ãŒ0ä»¥ä¸‹ã§ã™")
            return pd.Series()
        
        valid_weights = (valid_pnl['current_value_jpy'] / valid_total_value).values
        
        # é‡ã¿ã®ç¢ºèª
        logger.info(f"é‡ã¿åˆè¨ˆ: {valid_weights.sum():.6f}")
        for i, ticker in enumerate(valid_tickers):
            logger.info(f"é‡ã¿: {ticker} = {valid_weights[i]:.4f}")
        
        # åŠ é‡ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—ï¼ˆãƒªã‚¹ã‚¯åˆ†æã¨åŒã˜æ–¹å¼ï¼‰
        portfolio_returns = (returns_df[valid_tickers] * valid_weights).sum(axis=1)
        
        # æœ€çµ‚çš„ãªæ¬ æå€¤å‡¦ç†
        portfolio_returns = portfolio_returns.dropna()
        
        logger.info(f"ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—å®Œäº†: {len(portfolio_returns)}æ—¥åˆ†")
        logger.info(f"ãƒªã‚¿ãƒ¼ãƒ³çµ±è¨ˆ: å¹³å‡={portfolio_returns.mean():.6f}, æ¨™æº–åå·®={portfolio_returns.std():.6f}")
        
        if len(portfolio_returns) < 50:
            logger.warning(f"è¨ˆç®—ã•ã‚ŒãŸãƒªã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™: {len(portfolio_returns)}æ—¥")
        
        return portfolio_returns
        
    except Exception as e:
        logger.error(f"ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {error_details}")
        
        # Streamlitç”¨ã®ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
        try:
            import streamlit as st
            st.error(f"ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°"):
                st.code(error_details)
        except:
            pass
        
        return pd.Series()


# æ—§é–¢æ•°ã®äº’æ›æ€§ã®ãŸã‚ç¶­æŒ
def calculate_portfolio_returns(portfolio_df: pd.DataFrame, current_prices: Dict[str, float], period: str = '2y') -> pd.Series:
    """æ—§ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ç”¨ã®äº’æ›é–¢æ•°"""
    logger.warning("calculate_portfolio_returns is deprecated. Use calculate_portfolio_returns_robust instead.")
    return pd.Series()


def simple_ols_regression(y: pd.Series, X: pd.DataFrame) -> Dict[str, any]:
    """
    ç°¡å˜ãªOLSå›å¸°ï¼ˆstatsmodelsãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿ï¼‰
    
    Args:
        y: å¾“å±å¤‰æ•°
        X: èª¬æ˜å¤‰æ•°ï¼ˆå®šæ•°é …ã‚’å«ã‚€ï¼‰
    
    Returns:
        Dict: å›å¸°çµæœ
    """
    try:
        # è¡Œåˆ—è¨ˆç®—ã«ã‚ˆã‚‹æœ€å°äºŒä¹—æ³•
        X_array = X.values
        y_array = y.values
        
        # ãƒ™ãƒ¼ã‚¿è¨ˆç®—: Î² = (X'X)^-1 X'y
        XtX_inv = np.linalg.inv(X_array.T @ X_array)
        Xty = X_array.T @ y_array
        betas = XtX_inv @ Xty
        
        # äºˆæ¸¬å€¤ã¨æ®‹å·®
        y_pred = X_array @ betas
        residuals = y_array - y_pred
        
        # çµ±è¨ˆé‡è¨ˆç®—
        n = len(y)
        k = X.shape[1]
        
        # R squared
        ss_res = np.sum(residuals ** 2)
        ss_tot = np.sum((y_array - np.mean(y_array)) ** 2)
        r_squared = 1 - (ss_res / ss_tot)
        adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k)
        
        # æ¨™æº–èª¤å·®ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        mse = ss_res / (n - k)
        var_beta = mse * np.diag(XtX_inv)
        se_beta = np.sqrt(var_beta)
        
        # tçµ±è¨ˆé‡ã¨på€¤ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        t_stats = betas / se_beta
        # ç°¡æ˜“çš„ãªpå€¤è¨ˆç®—ï¼ˆæ­£ç¢ºã§ã¯ãªã„ãŒè¿‘ä¼¼ï¼‰
        p_values = 2 * (1 - np.abs(t_stats) / (np.abs(t_stats) + 1))
        
        # çµæœã‚’è¾æ›¸ã«ã¾ã¨ã‚
        params = pd.Series(betas, index=X.columns)
        pvalues = pd.Series(p_values, index=X.columns)
        tvalues = pd.Series(t_stats, index=X.columns)
        
        return {
            'params': params,
            'pvalues': pvalues,
            'tvalues': tvalues,
            'rsquared': r_squared,
            'rsquared_adj': adj_r_squared,
            'resid': pd.Series(residuals, index=y.index),
            'fittedvalues': pd.Series(y_pred, index=y.index),
            'fvalue': 0,  # Fçµ±è¨ˆé‡ã¯ç°¡ç•¥åŒ–
            'f_pvalue': 0.05  # ç°¡ç•¥åŒ–
        }
        
    except Exception as e:
        logger.error(f"ç°¡æ˜“å›å¸°è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}


def perform_factor_regression(
    portfolio_returns: pd.Series,
    factor_data: pd.DataFrame
) -> Dict[str, any]:
    """
    ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å›å¸°åˆ†æã‚’å®Ÿè¡Œ
    
    Args:
        portfolio_returns: ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒªã‚¿ãƒ¼ãƒ³
        factor_data: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
    
    Returns:
        Dict: å›å¸°åˆ†æçµæœï¼ˆãƒ™ãƒ¼ã‚¿ã€ã‚¢ãƒ«ãƒ•ã‚¡ã€çµ±è¨ˆé‡ãªã©ï¼‰
    """
    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆå…±é€šã®æ—¥ä»˜ã®ã¿ï¼‰
        df = pd.concat([
            portfolio_returns.rename('Portfolio'),
            factor_data
        ], axis=1).dropna()
        
        if df.empty:
            logger.error("ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å›å¸°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³")
            return {}
        
        # è¶…éãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—ï¼ˆãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒªã‚¿ãƒ¼ãƒ³ - ãƒªã‚¹ã‚¯ãƒ•ãƒªãƒ¼ãƒ¬ãƒ¼ãƒˆï¼‰
        excess_portfolio_returns = df['Portfolio'] - df['RF']
        
        # èª¬æ˜å¤‰æ•°ï¼ˆãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼‰
        X = df[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom']].copy()
        
        if STATSMODELS_AVAILABLE:
            # statsmodelsã‚’ä½¿ç”¨
            X = sm.add_constant(X)  # å®šæ•°é …ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ï¼‰ã‚’è¿½åŠ 
            model = sm.OLS(excess_portfolio_returns, X).fit()
            
            # çµæœã‚’æ•´ç†
            results = {
                'model': model,
                'betas': model.params.drop('const').to_dict(),
                'alpha': model.params['const'],
                'alpha_pvalue': model.pvalues['const'],
                'r_squared': model.rsquared,
                'adj_r_squared': model.rsquared_adj,
                'f_statistic': model.fvalue,
                'f_pvalue': model.f_pvalue,
                'n_observations': len(df),
                'factor_pvalues': model.pvalues.drop('const').to_dict(),
                'factor_tvalues': model.tvalues.drop('const').to_dict(),
                'residuals': model.resid,
                'fitted_values': model.fittedvalues
            }
        else:
            # ä»£æ›¿å®Ÿè£…ã‚’ä½¿ç”¨
            X['const'] = 1  # å®šæ•°é …ã‚’è¿½åŠ 
            X = X[['const'] + [col for col in X.columns if col != 'const']]  # å®šæ•°é …ã‚’å…ˆé ­ã«
            
            model_result = simple_ols_regression(excess_portfolio_returns, X)
            
            if model_result:
                factor_names = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom']
                results = {
                    'model': MockModel(),
                    'betas': {name: model_result['params'][name] for name in factor_names if name in model_result['params']},
                    'alpha': model_result['params']['const'],
                    'alpha_pvalue': model_result['pvalues']['const'],
                    'r_squared': model_result['rsquared'],
                    'adj_r_squared': model_result['rsquared_adj'],
                    'f_statistic': model_result['fvalue'],
                    'f_pvalue': model_result['f_pvalue'],
                    'n_observations': len(df),
                    'factor_pvalues': {name: model_result['pvalues'][name] for name in factor_names if name in model_result['pvalues']},
                    'factor_tvalues': {name: model_result['tvalues'][name] for name in factor_names if name in model_result['tvalues']},
                    'residuals': model_result['resid'],
                    'fitted_values': model_result['fittedvalues']
                }
            else:
                return {}
        
        logger.info(f"ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å›å¸°å®Œäº†: RÂ² = {results['r_squared']:.4f}")
        return results
        
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å›å¸°ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}


def calculate_rolling_betas(
    portfolio_returns: pd.Series,
    factor_data: pd.DataFrame,
    window: int = 21
) -> pd.DataFrame:
    """
    ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¿ã‚’è¨ˆç®—
    
    Args:
        portfolio_returns: ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒªã‚¿ãƒ¼ãƒ³
        factor_data: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
        window: ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆå–¶æ¥­æ—¥æ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ21æ—¥=ç´„1ãƒ¶æœˆï¼‰
    
    Returns:
        pd.DataFrame: ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—
    """
    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        df = pd.concat([
            portfolio_returns.rename('Portfolio'),
            factor_data
        ], axis=1).dropna()
        
        if len(df) < window:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ï¼ˆ{len(df)}æ—¥ï¼‰ã€ãƒ­ãƒ¼ãƒªãƒ³ã‚°è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€ä½{window}æ—¥å¿…è¦ï¼‰")
            return pd.DataFrame()
        
        # è¶…éãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—
        excess_portfolio_returns = df['Portfolio'] - df['RF']
        
        # èª¬æ˜å¤‰æ•°
        factor_names = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom']
        X = df[factor_names]
        
        if STATSMODELS_AVAILABLE:
            # statsmodelsã‚’ä½¿ç”¨
            X_with_const = sm.add_constant(X)
            rolling_model = RollingOLS(excess_portfolio_returns, X_with_const, window=window).fit()
            rolling_betas = rolling_model.params.drop(columns='const')
        else:
            # ä»£æ›¿å®Ÿè£…ï¼šç°¡æ˜“ãƒ­ãƒ¼ãƒªãƒ³ã‚°å›å¸°
            rolling_betas = pd.DataFrame(index=df.index[window-1:], columns=factor_names)
            
            for i in range(window-1, len(df)):
                start_idx = i - window + 1
                end_idx = i + 1
                
                # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ãƒ‡ãƒ¼ã‚¿
                y_window = excess_portfolio_returns.iloc[start_idx:end_idx]
                X_window = X.iloc[start_idx:end_idx].copy()
                X_window['const'] = 1
                X_window = X_window[['const'] + factor_names]
                
                # å›å¸°è¨ˆç®—
                try:
                    model_result = simple_ols_regression(y_window, X_window)
                    if model_result:
                        for factor in factor_names:
                            if factor in model_result['params']:
                                rolling_betas.loc[df.index[i], factor] = model_result['params'][factor]
                except:
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯NaN
                    pass
        
        logger.info(f"ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¿è¨ˆç®—å®Œäº†: {len(rolling_betas)}æœŸé–“ï¼ˆ{window}æ—¥çª“ï¼‰")
        return rolling_betas
        
    except Exception as e:
        logger.error(f"ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¿è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return pd.DataFrame()


def calculate_factor_contributions(
    factor_data: pd.DataFrame,
    betas: Dict[str, float]
) -> pd.DataFrame:
    """
    å„ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®å¯„ä¸åº¦ã‚’è¨ˆç®—
    
    Args:
        factor_data: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
        betas: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¿
    
    Returns:
        pd.DataFrame: æ—¥æ¬¡ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å¯„ä¸åº¦
    """
    try:
        contributions = pd.DataFrame(index=factor_data.index)
        
        for factor, beta in betas.items():
            if factor in factor_data.columns:
                contributions[factor] = factor_data[factor] * beta
        
        logger.info(f"ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å¯„ä¸åº¦è¨ˆç®—å®Œäº†: {len(contributions)}æ—¥åˆ†")
        return contributions
        
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å¯„ä¸åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return pd.DataFrame()


def get_factor_interpretation(factor_name: str, beta_value: float) -> str:
    """
    ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¿ã®è§£é‡ˆã‚’è¿”ã™
    
    Args:
        factor_name: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å
        beta_value: ãƒ™ãƒ¼ã‚¿å€¤
    
    Returns:
        str: è§£é‡ˆãƒ†ã‚­ã‚¹ãƒˆ
    """
    interpretations = {
        'Mkt-RF': {
            'high': 'å¸‚å ´ãƒªã‚¹ã‚¯ã«å¯¾ã—æ”»æ’ƒçš„ï¼ˆãƒã‚¤ãƒ™ãƒ¼ã‚¿ï¼‰',
            'low': 'å¸‚å ´ãƒªã‚¹ã‚¯ã«å¯¾ã—å®ˆå‚™çš„ï¼ˆãƒ­ãƒ¼ãƒ™ãƒ¼ã‚¿ï¼‰',
            'neutral': 'å¸‚å ´ãƒªã‚¹ã‚¯ã¨åŒç¨‹åº¦'
        },
        'SMB': {
            'high': 'å°å‹æ ªãƒã‚¤ã‚¢ã‚¹ï¼ˆå°å‹æ ªåŠ¹æœã‚’äº«å—ï¼‰',
            'low': 'å¤§å‹æ ªãƒã‚¤ã‚¢ã‚¹ï¼ˆå°å‹æ ªåŠ¹æœã«å¯¾ã—é€†ç›¸é–¢ï¼‰',
            'neutral': 'è¦æ¨¡ã«å¯¾ã—ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«'
        },
        'HML': {
            'high': 'ãƒãƒªãƒ¥ãƒ¼æ ªãƒã‚¤ã‚¢ã‚¹ï¼ˆå‰²å®‰æ ªé¸å¥½ï¼‰',
            'low': 'ã‚°ãƒ­ãƒ¼ã‚¹æ ªãƒã‚¤ã‚¢ã‚¹ï¼ˆæˆé•·æ ªé¸å¥½ï¼‰',
            'neutral': 'ãƒãƒªãƒ¥ãƒ¼/ã‚°ãƒ­ãƒ¼ã‚¹ã«å¯¾ã—ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«'
        },
        'RMW': {
            'high': 'é«˜åç›Šæ€§ä¼æ¥­ã¸ã®å‚¾æ–œ',
            'low': 'ä½åç›Šæ€§ä¼æ¥­ã¸ã®å‚¾æ–œ',
            'neutral': 'åç›Šæ€§ã«å¯¾ã—ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«'
        },
        'CMA': {
            'high': 'ä¿å®ˆçš„æŠ•è³‡ä¼æ¥­ã¸ã®å‚¾æ–œ',
            'low': 'ç©æ¥µçš„æŠ•è³‡ä¼æ¥­ã¸ã®å‚¾æ–œ',
            'neutral': 'æŠ•è³‡å§¿å‹¢ã«å¯¾ã—ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«'
        },
        'Mom': {
            'high': 'ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ åŠ¹æœã‚’äº«å—ï¼ˆä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰è¿½éšï¼‰',
            'low': 'ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ åŠ¹æœã«å¯¾ã—é€†ç›¸é–¢ï¼ˆé€†å¼µã‚Šï¼‰',
            'neutral': 'ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã«å¯¾ã—ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«'
        }
    }
    
    if factor_name not in interpretations:
        return f'{factor_name}: {beta_value:.3f}'
    
    if beta_value > 0.2:
        category = 'high'
    elif beta_value < -0.2:
        category = 'low'
    else:
        category = 'neutral'
    
    return interpretations[factor_name][category]