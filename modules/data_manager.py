"""
ä¸­å¤®é›†ç´„å‹ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æã«å¿…è¦ãªå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å–å¾—ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
"""

import pandas as pd
import numpy as np
import yfinance as yf
import streamlit as st
from typing import Dict, List, Optional, Tuple, Any
import logging
import time
import os
from datetime import datetime, timedelta
import json
import pickle
from pathlib import Path

logger = logging.getLogger(__name__)


class DataManager:
    """
    ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æç”¨ã®ä¸­å¤®é›†ç´„å‹ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    å…¨ã¦ã®å¤–éƒ¨APIå‘¼ã³å‡ºã—ã‚’ç®¡ç†ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æä¾›
    """
    
    def __init__(self, cache_dir: str = "data_cache"):
        """
        ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
        
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.memory_cache = {}
        self.cache_timestamps = {}
        
        # ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æœŸé™ï¼ˆç§’ï¼‰
        self.cache_expiry = {
            'current_prices': 300,      # 5åˆ†
            'exchange_rates': 900,      # 15åˆ†
            'company_info': 3600,       # 1æ™‚é–“
            'historical_prices': 3600,  # 1æ™‚é–“
            'factor_data': 86400,       # 24æ™‚é–“
            'etf_benchmarks': 3600      # 1æ™‚é–“
        }
        
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    
    def load_portfolio_data(self, portfolio_df: pd.DataFrame) -> Dict[str, Any]:
        """
        ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã«å¿…è¦ãªå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å–å¾—
        
        Args:
            portfolio_df: ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            
        Returns:
            Dict: å…¨ã¦ã®å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸
        """
        logger.info(f"ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—é–‹å§‹: {len(portfolio_df)}éŠ˜æŸ„")
        
        # ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã‚’å–å¾—
        tickers = portfolio_df['Ticker'].tolist()
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¨­å®š
        try:
            progress_bar = st.progress(0)
            status_text = st.empty()
            show_progress = True
        except:
            show_progress = False
        
        data_bundle = {}
        
        # 1. ç¾åœ¨æ ªä¾¡å–å¾— (20%)
        if show_progress:
            progress_bar.progress(0.1)
            status_text.text("ç¾åœ¨æ ªä¾¡ã‚’å–å¾—ä¸­...")
        
        data_bundle['current_prices'] = self.get_current_prices(tickers)
        
        # 2. ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆå–å¾— (30%)
        if show_progress:
            progress_bar.progress(0.2)
            status_text.text("ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—ä¸­...")
        
        data_bundle['exchange_rates'] = self.get_exchange_rates()
        
        # 3. ä¼æ¥­æƒ…å ±å–å¾— (50%)
        if show_progress:
            progress_bar.progress(0.3)
            status_text.text("ä¼æ¥­æƒ…å ±ã‚’å–å¾—ä¸­...")
        
        data_bundle['company_info'] = self.get_company_info_batch(tickers)
        
        # 4. éå»5å¹´åˆ†ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾— (70%)
        if show_progress:
            progress_bar.progress(0.5)
            status_text.text("éå»5å¹´åˆ†ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        
        data_bundle['historical_prices'] = self.get_historical_prices_batch(tickers, period="5y")
        
        # 5. Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾— (85%)
        if show_progress:
            progress_bar.progress(0.7)
            status_text.text("Fama-French 5å¹´åˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        
        # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã¯å¿…ãšæœ€æ–°ã®éå»5å¹´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        data_bundle['factor_data'] = self.get_factor_data(force_refresh=True)
        
        # 6. ETFãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾— (95%)
        if show_progress:
            progress_bar.progress(0.85)
            status_text.text("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        
        data_bundle['etf_benchmarks'] = self.get_etf_benchmark_data()
        
        # 7. é€šè²¨ãƒãƒƒãƒ”ãƒ³ã‚°ç”Ÿæˆ (100%)
        if show_progress:
            progress_bar.progress(0.95)
            status_text.text("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Œäº†ä¸­...")
        
        data_bundle['currency_mapping'] = self.create_currency_mapping(tickers)
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        data_bundle['data_quality'] = self.assess_data_quality(data_bundle, tickers)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.save_data_bundle(data_bundle, tickers)
        
        if show_progress:
            progress_bar.progress(1.0)
            status_text.text(f"ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(tickers)}éŠ˜æŸ„")
            time.sleep(1)
            progress_bar.empty()
            status_text.empty()
        
        logger.info("ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—å®Œäº†")
        return data_bundle
    
    
    def get_current_prices(self, tickers: List[str]) -> Dict[str, float]:
        """
        ç¾åœ¨æ ªä¾¡ã‚’ä¸€æ‹¬å–å¾—
        """
        cache_key = f"current_prices_{hash(tuple(sorted(tickers)))}"
        
        if self.is_cache_valid(cache_key, 'current_prices'):
            logger.info("ç¾åœ¨æ ªä¾¡ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
            return self.memory_cache[cache_key]
        
        logger.info(f"ç¾åœ¨æ ªä¾¡å–å¾—é–‹å§‹: {len(tickers)}éŠ˜æŸ„")
        
        prices = {}
        
        # ãƒãƒƒãƒå–å¾—ã‚’è©¦è¡Œ
        try:
            # 10éŠ˜æŸ„ãšã¤ãƒãƒƒãƒå‡¦ç†
            batch_size = 10
            for i in range(0, len(tickers), batch_size):
                batch = tickers[i:i + batch_size]
                logger.info(f"ä¾¡æ ¼å–å¾—ãƒãƒƒãƒ {i//batch_size + 1}: {batch}")
                
                try:
                    # yfinanceã®ãƒãƒƒãƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    batch_data = yf.download(batch, period="2d", interval="1d", 
                                           group_by='ticker', auto_adjust=True, 
                                           prepost=True, threads=True)
                    
                    if batch_data.empty:
                        logger.warning(f"ãƒãƒƒãƒ {i//batch_size + 1} ã§ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                        continue
                    
                    # å€‹åˆ¥éŠ˜æŸ„ã®ä¾¡æ ¼ã‚’æŠ½å‡º
                    for ticker in batch:
                        try:
                            if len(batch) == 1:
                                # å˜ä¸€éŠ˜æŸ„ã®å ´åˆ
                                ticker_data = batch_data
                            else:
                                # è¤‡æ•°éŠ˜æŸ„ã®å ´åˆ
                                ticker_data = batch_data[ticker] if ticker in batch_data.columns.levels[0] else None
                            
                            if ticker_data is not None and not ticker_data.empty:
                                # æœ€æ–°ã®çµ‚å€¤ã‚’å–å¾—
                                latest_price = ticker_data['Close'].dropna().iloc[-1]
                                prices[ticker] = float(latest_price)
                                logger.debug(f"ä¾¡æ ¼å–å¾—æˆåŠŸ: {ticker} = {latest_price}")
                            else:
                                logger.warning(f"ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãªã—: {ticker}")
                                prices[ticker] = 0.0
                                
                        except Exception as e:
                            logger.error(f"å€‹åˆ¥ä¾¡æ ¼å–å¾—ã‚¨ãƒ©ãƒ¼ {ticker}: {str(e)}")
                            prices[ticker] = 0.0
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                    if i + batch_size < len(tickers):
                        time.sleep(0.5)
                        
                except Exception as e:
                    logger.error(f"ãƒãƒƒãƒå–å¾—ã‚¨ãƒ©ãƒ¼ {i//batch_size + 1}: {str(e)}")
                    # å€‹åˆ¥å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    for ticker in batch:
                        try:
                            stock = yf.Ticker(ticker)
                            hist = stock.history(period="2d")
                            if not hist.empty:
                                prices[ticker] = float(hist['Close'].iloc[-1])
                            else:
                                prices[ticker] = 0.0
                        except:
                            prices[ticker] = 0.0
                        time.sleep(0.2)
        
        except Exception as e:
            logger.error(f"ç¾åœ¨æ ªä¾¡å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.memory_cache[cache_key] = prices
        self.cache_timestamps[cache_key] = time.time()
        
        success_count = len([p for p in prices.values() if p > 0])
        logger.info(f"ç¾åœ¨æ ªä¾¡å–å¾—å®Œäº†: {success_count}/{len(tickers)}éŠ˜æŸ„æˆåŠŸ")
        
        return prices
    
    
    def get_exchange_rates(self) -> Dict[str, float]:
        """
        ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
        """
        cache_key = "exchange_rates"
        
        if self.is_cache_valid(cache_key, 'exchange_rates'):
            logger.info("ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
            return self.memory_cache[cache_key]
        
        logger.info("ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆå–å¾—é–‹å§‹")
        
        currency_pairs = ['USDJPY=X', 'EURJPY=X', 'GBPJPY=X', 'AUDJPY=X', 'CADJPY=X', 'CHFJPY=X']
        rates = {}
        
        try:
            # ãƒãƒƒãƒã§ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
            fx_data = yf.download(currency_pairs, period="5d", interval="1d", 
                                group_by='ticker', auto_adjust=True, threads=True)
            
            if not fx_data.empty:
                for pair in currency_pairs:
                    try:
                        if len(currency_pairs) == 1:
                            pair_data = fx_data
                        else:
                            pair_data = fx_data[pair] if pair in fx_data.columns.levels[0] else None
                        
                        if pair_data is not None and not pair_data.empty:
                            latest_rate = pair_data['Close'].dropna().iloc[-1]
                            rates[pair] = float(latest_rate)
                        else:
                            logger.warning(f"ç‚ºæ›¿ãƒ‡ãƒ¼ã‚¿ãªã—: {pair}")
                            
                    except Exception as e:
                        logger.error(f"ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼ {pair}: {str(e)}")
            
        except Exception as e:
            logger.error(f"ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆãƒãƒƒãƒå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ã‚’è¨­å®š
        fallback_rates = {
            'USDJPY=X': 150.0,
            'EURJPY=X': 160.0,
            'GBPJPY=X': 180.0,
            'AUDJPY=X': 100.0,
            'CADJPY=X': 110.0,
            'CHFJPY=X': 165.0
        }
        
        for pair in currency_pairs:
            if pair not in rates:
                rates[pair] = fallback_rates[pair]
                logger.warning(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆä½¿ç”¨: {pair} = {fallback_rates[pair]}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.memory_cache[cache_key] = rates
        self.cache_timestamps[cache_key] = time.time()
        
        logger.info(f"ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆå–å¾—å®Œäº†: {len(rates)}é€šè²¨ãƒšã‚¢")
        return rates
    
    
    def get_company_info_batch(self, tickers: List[str]) -> Dict[str, Dict[str, Any]]:
        """
        ä¼æ¥­æƒ…å ±ã‚’ä¸€æ‹¬å–å¾—
        """
        cache_key = f"company_info_{hash(tuple(sorted(tickers)))}"
        
        if self.is_cache_valid(cache_key, 'company_info'):
            logger.info("ä¼æ¥­æƒ…å ±ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
            return self.memory_cache[cache_key]
        
        logger.info(f"ä¼æ¥­æƒ…å ±ä¸€æ‹¬å–å¾—é–‹å§‹: {len(tickers)}éŠ˜æŸ„")
        
        # æ—¢å­˜ã® country_fetcher ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨
        from modules.country_fetcher import get_multiple_ticker_complete_info
        
        # ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã‚‚æ¸¡ã™
        exchange_rates = self.get_exchange_rates()
        
        company_info = get_multiple_ticker_complete_info(tickers, exchange_rates)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.memory_cache[cache_key] = company_info
        self.cache_timestamps[cache_key] = time.time()
        
        logger.info(f"ä¼æ¥­æƒ…å ±ä¸€æ‹¬å–å¾—å®Œäº†: {len(company_info)}éŠ˜æŸ„")
        return company_info
    
    
    def get_historical_prices_batch(self, tickers: List[str], period: str = "5y") -> Dict[str, pd.DataFrame]:
        """
        éå»ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å–å¾—ï¼ˆæœ€å¤§5å¹´åˆ†ï¼‰
        """
        cache_key = f"historical_prices_{period}_{hash(tuple(sorted(tickers)))}"
        
        if self.is_cache_valid(cache_key, 'historical_prices'):
            logger.info(f"éå»æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ{period}ï¼‰ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
            return self.memory_cache[cache_key]
        
        logger.info(f"éå»æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {len(tickers)}éŠ˜æŸ„, æœŸé–“: {period}")
        
        historical_data = {}
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆãƒ¡ãƒ¢ãƒªã¨APIåˆ¶é™ã‚’è€ƒæ…®ï¼‰
        batch_size = 20
        
        for i in range(0, len(tickers), batch_size):
            batch = tickers[i:i + batch_size]
            logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒãƒƒãƒ {i//batch_size + 1}: {len(batch)}éŠ˜æŸ„")
            
            try:
                # yfinanceã§ãƒãƒƒãƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                batch_data = yf.download(batch, period=period, interval="1d", 
                                       group_by='ticker', auto_adjust=True, 
                                       prepost=False, threads=True)
                
                if batch_data.empty:
                    logger.warning(f"éå»ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ {i//batch_size + 1} ã§ë°ì´í„°ãªã—")
                    continue
                
                # å€‹åˆ¥éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                for ticker in batch:
                    try:
                        if len(batch) == 1:
                            ticker_data = batch_data
                        else:
                            ticker_data = batch_data[ticker] if ticker in batch_data.columns.levels[0] else pd.DataFrame()
                        
                        if not ticker_data.empty:
                            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                            ticker_data = ticker_data.dropna()
                            if len(ticker_data) > 0:
                                historical_data[ticker] = ticker_data
                                logger.debug(f"éå»ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {ticker} ({len(ticker_data)}æ—¥åˆ†)")
                            else:
                                logger.warning(f"éå»ãƒ‡ãƒ¼ã‚¿ãŒç©º: {ticker}")
                                historical_data[ticker] = pd.DataFrame()
                        else:
                            logger.warning(f"éå»ãƒ‡ãƒ¼ã‚¿ãªã—: {ticker}")
                            historical_data[ticker] = pd.DataFrame()
                            
                    except Exception as e:
                        logger.error(f"å€‹åˆ¥éå»ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ {ticker}: {str(e)}")
                        historical_data[ticker] = pd.DataFrame()
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                if i + batch_size < len(tickers):
                    time.sleep(1.0)
                    
            except Exception as e:
                logger.error(f"éå»ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒå–å¾—ã‚¨ãƒ©ãƒ¼ {i//batch_size + 1}: {str(e)}")
                # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§åŸ‹ã‚ã‚‹
                for ticker in batch:
                    historical_data[ticker] = pd.DataFrame()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.memory_cache[cache_key] = historical_data
        self.cache_timestamps[cache_key] = time.time()
        
        success_count = len([df for df in historical_data.values() if not df.empty])
        logger.info(f"éå»æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {success_count}/{len(tickers)}éŠ˜æŸ„æˆåŠŸ")
        
        return historical_data
    
    
    def get_factor_data(self, force_refresh: bool = False) -> Dict[str, pd.DataFrame]:
        """
        Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆéå»5å¹´åˆ†ï¼‰
        
        Args:
            force_refresh: å¼·åˆ¶çš„ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹
        """
        cache_key = "factor_data_5y"
        
        if not force_refresh and self.is_cache_valid(cache_key, 'factor_data'):
            logger.info("ğŸ“¦ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
            return self.memory_cache[cache_key]
        
        logger.info("ğŸ¯ Fama-French 5å¹´åˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        
        # å …ç‰¢ãªdirect downloadã‚’ä½¿ç”¨
        from modules.factor_analysis import download_fama_french_direct, get_fama_french_factors
        
        try:
            # éå»5å¹´åˆ†ã®æœŸé–“ã‚’è¨ˆç®—ï¼ˆå°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹ï¼‰
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=5*365 + 30)).strftime('%Y-%m-%d')  # 30æ—¥ä½™è£•
            
            logger.info(f"ğŸ“… å–å¾—æœŸé–“: {start_date} ï½ {end_date}")
            
            # 1. ã¾ãšå …ç‰¢ãªç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œ
            try:
                logger.info("ğŸ¯ Kenneth Frenchå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ...")
                factor_dataframe = download_fama_french_direct(start_date, end_date)
                
                if isinstance(factor_dataframe, pd.DataFrame) and not factor_dataframe.empty and len(factor_dataframe) > 500:
                    factor_data = {'FF5_Factors': factor_dataframe}
                    
                    # Streamlité€šçŸ¥
                    try:
                        import streamlit as st
                        st.success(f"âœ… **Kenneth Frenchå…¬å¼ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸï¼**\n\n"
                                f"- ğŸ“Š ãƒ‡ãƒ¼ã‚¿æœŸé–“: {factor_dataframe.index.min().strftime('%Y-%m-%d')} ï½ {factor_dataframe.index.max().strftime('%Y-%m-%d')}\n"
                                f"- ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿æ•°: {len(factor_dataframe):,}å–¶æ¥­æ—¥åˆ†\n"
                                f"- ğŸ” ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: {', '.join(factor_dataframe.columns)}\n"
                                f"- ğŸ¯ å®Ÿéš›ã®å­¦è¡“ç ”ç©¶ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ä¸­")
                    except:
                        pass
                    
                    logger.info(f"âœ… ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {len(factor_dataframe)}æ—¥åˆ†")
                    
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    self.memory_cache[cache_key] = factor_data
                    self.cache_timestamps[cache_key] = time.time()
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚ä¿å­˜
                    self.save_factor_data_to_file(factor_data, start_date, end_date)
                    
                    return factor_data
                else:
                    raise ValueError("ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {str(e)}")
                logger.info("ğŸ“¡ pandas-datareaderã§ã®å–å¾—ã‚’è©¦è¡Œ...")
            
            # 2. pandas-datareaderã§ã®å–å¾—ã‚’è©¦è¡Œ
            try:
                factor_dataframe = get_fama_french_factors(start_date, end_date)
                
                if isinstance(factor_dataframe, pd.DataFrame) and not factor_dataframe.empty and len(factor_dataframe) > 500:
                    factor_data = {'FF5_Factors': factor_dataframe}
                    
                    # Streamlité€šçŸ¥
                    try:
                        import streamlit as st
                        st.info(f"âœ… **Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ**\n\n"
                               f"- ğŸ“Š ãƒ‡ãƒ¼ã‚¿æœŸé–“: {factor_dataframe.index.min().strftime('%Y-%m-%d')} ï½ {factor_dataframe.index.max().strftime('%Y-%m-%d')}\n"
                               f"- ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿æ•°: {len(factor_dataframe):,}å–¶æ¥­æ—¥åˆ†\n"
                               f"- ğŸ“¡ pandas-datareaderçµŒç”±ã§å–å¾—")
                    except:
                        pass
                    
                    logger.info(f"âœ… pandas-datareaderæˆåŠŸ: {len(factor_dataframe)}æ—¥åˆ†")
                    
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    self.memory_cache[cache_key] = factor_data
                    self.cache_timestamps[cache_key] = time.time()
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚ä¿å­˜
                    self.save_factor_data_to_file(factor_data, start_date, end_date)
                    
                    return factor_data
                else:
                    raise ValueError("pandas-datareaderã§ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ pandas-datareaderå¤±æ•—: {str(e)}")
                logger.info("ğŸ“ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ...")
            
            # 3. ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
            try:
                cached_factor_data = self.load_factor_data_from_file()
                if cached_factor_data:
                    logger.info("âœ… ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿")
                    
                    try:
                        import streamlit as st
                        st.warning("ğŸ“ **ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ä¸­**\n\n"
                                 "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®å•é¡Œã«ã‚ˆã‚Šã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n"
                                 "éå»ã«ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚")
                    except:
                        pass
                    
                    self.memory_cache[cache_key] = cached_factor_data
                    self.cache_timestamps[cache_key] = time.time()
                    
                    return cached_factor_data
            except Exception as e:
                logger.warning(f"âš ï¸ ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {str(e)}")
            
            # 4. æœ€çµ‚æ‰‹æ®µï¼šçµ±è¨ˆçš„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
            logger.warning("ğŸ”„ å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã€çµ±è¨ˆçš„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
            sample_data = self.create_sample_factor_data(start_date, end_date)
            
            try:
                import streamlit as st
                st.error("âš ï¸ **ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—**\n\n"
                        "å®Ÿéš›ã®Fama-Frenchãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n"
                        "çµ±è¨ˆçš„ã«ç¾å®Ÿçš„ãªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n\n"
                        "**å¯¾å‡¦æ–¹æ³•:**\n"
                        "1. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª\n"
                        "2. `pip install pandas-datareader`ã‚’å®Ÿè¡Œ\n"
                        "3. ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿")
            except:
                pass
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.memory_cache[cache_key] = sample_data
            self.cache_timestamps[cache_key] = time.time()
            
            return sample_data
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                end_date = datetime.now().strftime('%Y-%m-%d')
                start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d')
                minimal_data = self.create_sample_factor_data(start_date, end_date)
                
                self.memory_cache[cache_key] = minimal_data
                self.cache_timestamps[cache_key] = time.time()
                
                return minimal_data
            except:
                return {}
    
    
    def get_etf_benchmark_data(self) -> Dict[str, Dict[str, Optional[float]]]:
        """
        ETFãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        """
        cache_key = "etf_benchmarks"
        
        if self.is_cache_valid(cache_key, 'etf_benchmarks'):
            logger.info("ETFãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
            return self.memory_cache[cache_key]
        
        logger.info("ETFãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        
        # æ—¢å­˜ã® pnl_calculator ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        from modules.pnl_calculator import get_etf_benchmark_data
        
        try:
            benchmark_data = get_etf_benchmark_data()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.memory_cache[cache_key] = benchmark_data
            self.cache_timestamps[cache_key] = time.time()
            
            logger.info("ETFãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
            return benchmark_data
            
        except Exception as e:
            logger.error(f"ETFãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}
    
    
    def create_currency_mapping(self, tickers: List[str]) -> Dict[str, str]:
        """
        ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã‹ã‚‰é€šè²¨ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        """
        currency_mapping = {}
        
        for ticker in tickers:
            if '.T' in ticker or '.JP' in ticker:
                currency_mapping[ticker] = 'JPY'
            elif '.L' in ticker:
                currency_mapping[ticker] = 'GBP'
            elif '.PA' in ticker or '.DE' in ticker or '.MI' in ticker or '.AS' in ticker:
                currency_mapping[ticker] = 'EUR'
            elif '.TO' in ticker or '.V' in ticker:
                currency_mapping[ticker] = 'CAD'
            elif '.AX' in ticker:
                currency_mapping[ticker] = 'AUD'
            elif '.SW' in ticker:
                currency_mapping[ticker] = 'CHF'
            else:
                currency_mapping[ticker] = 'USD'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        logger.info(f"é€šè²¨ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆå®Œäº†: {len(currency_mapping)}éŠ˜æŸ„")
        return currency_mapping
    
    
    def assess_data_quality(self, data_bundle: Dict[str, Any], tickers: List[str]) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’è©•ä¾¡
        """
        quality_report = {
            'total_tickers': len(tickers),
            'price_success_rate': 0,
            'company_info_success_rate': 0,
            'historical_data_success_rate': 0,
            'missing_data': [],
            'data_freshness': datetime.now().isoformat()
        }
        
        try:
            # ç¾åœ¨ä¾¡æ ¼ã®æˆåŠŸç‡
            price_success = len([p for p in data_bundle['current_prices'].values() if p > 0])
            quality_report['price_success_rate'] = price_success / len(tickers) * 100
            
            # ä¼æ¥­æƒ…å ±ã®æˆåŠŸç‡
            company_success = len([info for info in data_bundle['company_info'].values() 
                                 if info and (info.get('country') or info.get('sector'))])
            quality_report['company_info_success_rate'] = company_success / len(tickers) * 100
            
            # éå»ãƒ‡ãƒ¼ã‚¿ã®æˆåŠŸç‡
            historical_success = len([df for df in data_bundle['historical_prices'].values() if not df.empty])
            quality_report['historical_data_success_rate'] = historical_success / len(tickers) * 100
            
            # ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å®š
            for ticker in tickers:
                issues = []
                if data_bundle['current_prices'].get(ticker, 0) <= 0:
                    issues.append('price')
                if not data_bundle['company_info'].get(ticker):
                    issues.append('company_info')
                if data_bundle['historical_prices'].get(ticker, pd.DataFrame()).empty:
                    issues.append('historical_data')
                
                if issues:
                    quality_report['missing_data'].append({
                        'ticker': ticker,
                        'missing': issues
                    })
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡å®Œäº†: ä¾¡æ ¼ {quality_report['price_success_rate']:.1f}%, "
                   f"ä¼æ¥­æƒ…å ± {quality_report['company_info_success_rate']:.1f}%, "
                   f"éå»ãƒ‡ãƒ¼ã‚¿ {quality_report['historical_data_success_rate']:.1f}%")
        
        return quality_report
    
    
    def is_cache_valid(self, cache_key: str, data_type: str) -> bool:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        """
        if cache_key not in self.memory_cache or cache_key not in self.cache_timestamps:
            return False
        
        elapsed = time.time() - self.cache_timestamps[cache_key]
        expiry = self.cache_expiry.get(data_type, 3600)
        
        return elapsed < expiry
    
    
    def save_data_bundle(self, data_bundle: Dict[str, Any], tickers: List[str]):
        """
        ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆPickleã¨CSVä¸¡æ–¹ï¼‰
        """
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜ã¨ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒãƒƒã‚·ãƒ¥ã‚’å«ã‚ã‚‹
            ticker_hash = hash(tuple(sorted(tickers)))
            date_str = datetime.now().strftime('%Y%m%d')
            base_filename = f"data_bundle_{date_str}_{abs(ticker_hash)}"
            
            # Pickleãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
            pickle_filepath = self.cache_dir / f"{base_filename}.pkl"
            with open(pickle_filepath, 'wb') as f:
                pickle.dump(data_bundle, f)
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒ«ä¿å­˜å®Œäº†: {pickle_filepath}")
            
            # CSVå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³åˆ©ç”¨ãƒ»åˆ†æç”¨ï¼‰
            self.save_data_bundle_as_csv(data_bundle, tickers, date_str, abs(ticker_hash))
            
            # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ7æ—¥ä»¥ä¸Šå¤ã„ï¼‰
            self.cleanup_old_cache_files(days=7)
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    
    def save_data_bundle_as_csv(self, data_bundle: Dict[str, Any], tickers: List[str], date_str: str, ticker_hash: int):
        """
        ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒ«ã‚’CSVå½¢å¼ã§ä¿å­˜
        """
        try:
            csv_dir = self.cache_dir / "csv_exports" / f"{date_str}_{ticker_hash}"
            csv_dir.mkdir(parents=True, exist_ok=True)
            
            # 1. ç¾åœ¨æ ªä¾¡
            if 'current_prices' in data_bundle:
                prices_df = pd.DataFrame([
                    {'Ticker': ticker, 'CurrentPrice': price}
                    for ticker, price in data_bundle['current_prices'].items()
                ])
                prices_df.to_csv(csv_dir / "current_prices.csv", index=False)
            
            # 2. ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ
            if 'exchange_rates' in data_bundle:
                fx_df = pd.DataFrame([
                    {'CurrencyPair': pair, 'Rate': rate}
                    for pair, rate in data_bundle['exchange_rates'].items()
                ])
                fx_df.to_csv(csv_dir / "exchange_rates.csv", index=False)
            
            # 3. ä¼æ¥­æƒ…å ±
            if 'company_info' in data_bundle:
                company_data = []
                for ticker, info in data_bundle['company_info'].items():
                    if info:
                        row = {'Ticker': ticker}
                        row.update(info)
                        company_data.append(row)
                
                if company_data:
                    company_df = pd.DataFrame(company_data)
                    company_df.to_csv(csv_dir / "company_info.csv", index=False)
            
            # 4. éå»æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆå„éŠ˜æŸ„åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
            if 'historical_prices' in data_bundle:
                hist_dir = csv_dir / "historical_prices"
                hist_dir.mkdir(exist_ok=True)
                
                for ticker, df in data_bundle['historical_prices'].items():
                    if not df.empty:
                        # ãƒ†ã‚£ãƒƒã‚«ãƒ¼åã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«å®‰å…¨ãªå½¢å¼ã«å¤‰æ›
                        safe_ticker = ticker.replace('.', '_').replace('/', '_')
                        df.to_csv(hist_dir / f"{safe_ticker}_historical.csv")
            
            # 5. ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
            if 'factor_data' in data_bundle:
                factor_dir = csv_dir / "factor_data"
                factor_dir.mkdir(exist_ok=True)
                
                for factor_name, df in data_bundle['factor_data'].items():
                    if isinstance(df, pd.DataFrame) and not df.empty:
                        df.to_csv(factor_dir / f"{factor_name}.csv")
            
            # 6. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
            if 'data_quality' in data_bundle:
                quality_df = pd.DataFrame([data_bundle['data_quality']])
                quality_df.to_csv(csv_dir / "data_quality_report.csv", index=False)
            
            # 7. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
            metadata = {
                'export_date': datetime.now().isoformat(),
                'tickers': tickers,
                'ticker_count': len(tickers),
                'data_types': list(data_bundle.keys())
            }
            
            with open(csv_dir / "metadata.json", 'w') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            logger.info(f"CSVå½¢å¼ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {csv_dir}")
            
        except Exception as e:
            logger.error(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    
    def load_data_bundle(self, tickers: List[str]) -> Optional[Dict[str, Any]]:
        """
        ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒ«ã‚’èª­ã¿è¾¼ã¿
        """
        try:
            ticker_hash = hash(tuple(sorted(tickers)))
            date_str = datetime.now().strftime('%Y%m%d')
            filename = f"data_bundle_{date_str}_{abs(ticker_hash)}.pkl"
            filepath = self.cache_dir / filename
            
            if filepath.exists():
                with open(filepath, 'rb') as f:
                    data_bundle = pickle.load(f)
                logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {filepath}")
                return data_bundle
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return None
    
    
    def cleanup_old_cache_files(self, days: int = 7):
        """
        å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            for filepath in self.cache_dir.glob("data_bundle_*.pkl"):
                if filepath.stat().st_mtime < cutoff_date.timestamp():
                    filepath.unlink()
                    logger.debug(f"å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {filepath}")
                    
        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    
    def get_data_freshness_info(self) -> Dict[str, str]:
        """
        ãƒ‡ãƒ¼ã‚¿ã®æ–°é®®åº¦æƒ…å ±ã‚’å–å¾—
        """
        freshness_info = {}
        
        for data_type, cache_key in [
            ('current_prices', 'current_prices'),
            ('exchange_rates', 'exchange_rates'),
            ('company_info', 'company_info'),
            ('factor_data', 'factor_data')
        ]:
            if cache_key in self.cache_timestamps:
                timestamp = self.cache_timestamps[cache_key]
                age_seconds = time.time() - timestamp
                
                if age_seconds < 60:
                    freshness_info[data_type] = f"{int(age_seconds)}ç§’å‰"
                elif age_seconds < 3600:
                    freshness_info[data_type] = f"{int(age_seconds/60)}åˆ†å‰"
                else:
                    freshness_info[data_type] = f"{int(age_seconds/3600)}æ™‚é–“å‰"
            else:
                freshness_info[data_type] = "æœªå–å¾—"
        
        return freshness_info
    
    
    def save_factor_data_to_file(self, factor_data: Dict[str, pd.DataFrame], start_date: str, end_date: str):
        """
        ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        """
        try:
            factor_cache_dir = self.cache_dir / "factor_data"
            factor_cache_dir.mkdir(exist_ok=True)
            
            # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«å
            date_str = datetime.now().strftime('%Y%m%d')
            
            for factor_name, df in factor_data.items():
                if isinstance(df, pd.DataFrame) and not df.empty:
                    # CSVã¨ã—ã¦ä¿å­˜
                    csv_path = factor_cache_dir / f"{factor_name}_{date_str}.csv"
                    df.to_csv(csv_path)
                    
                    # Pickleã¨ã—ã¦ã‚‚ä¿å­˜ï¼ˆã‚ˆã‚Šå®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ä¿æŒï¼‰
                    pickle_path = factor_cache_dir / f"{factor_name}_{date_str}.pkl"
                    df.to_pickle(pickle_path)
                    
                    logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {csv_path}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
            metadata = {
                'save_date': datetime.now().isoformat(),
                'start_date': start_date,
                'end_date': end_date,
                'data_types': list(factor_data.keys()),
                'total_days': sum(len(df) for df in factor_data.values() if isinstance(df, pd.DataFrame))
            }
            
            metadata_path = factor_cache_dir / f"metadata_{date_str}.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    
    def load_factor_data_from_file(self) -> Optional[Dict[str, pd.DataFrame]]:
        """
        ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        """
        try:
            factor_cache_dir = self.cache_dir / "factor_data"
            if not factor_cache_dir.exists():
                return None
            
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            pickle_files = list(factor_cache_dir.glob("FF5_Factors_*.pkl"))
            if not pickle_files:
                return None
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã—ã¦ã‚½ãƒ¼ãƒˆ
            latest_file = max(pickle_files, key=lambda x: x.stem.split('_')[-1])
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–°ã—ã•ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ7æ—¥ä»¥å†…ï¼‰
            file_age = time.time() - latest_file.stat().st_mtime
            if file_age > 7 * 24 * 3600:  # 7æ—¥ä»¥ä¸Šå¤ã„
                logger.warning("ğŸ“ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå¤ã™ãã¾ã™ï¼ˆ7æ—¥ä»¥ä¸Šï¼‰")
                return None
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            factor_df = pd.read_pickle(latest_file)
            
            if isinstance(factor_df, pd.DataFrame) and not factor_df.empty:
                logger.info(f"ğŸ“ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(factor_df)}æ—¥åˆ†")
                return {'FF5_Factors': factor_df}
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return None
    
    
    def create_sample_factor_data(self, start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """
        ã‚µãƒ³ãƒ—ãƒ«Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        """
        try:
            import pandas as pd
            import numpy as np
            from datetime import datetime, timedelta
            
            # æ—¥ä»˜ç¯„å›²ã‚’ç”Ÿæˆ
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')
            
            # å–¶æ¥­æ—¥ã®ã¿ç”Ÿæˆï¼ˆåœŸæ—¥ã‚’é™¤ãï¼‰
            date_range = pd.bdate_range(start=start_dt, end=end_dt)
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
            n_days = len(date_range)
            
            # Fama-French 5ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ + Momentum
            factor_data = {
                'Mkt-RF': np.random.normal(0.0005, 0.012, n_days),  # å¸‚å ´ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
                'SMB': np.random.normal(0.0001, 0.008, n_days),     # å°å‹æ ªãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
                'HML': np.random.normal(0.0002, 0.009, n_days),     # ãƒãƒªãƒ¥ãƒ¼ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
                'RMW': np.random.normal(0.0001, 0.007, n_days),     # åç›Šæ€§ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
                'CMA': np.random.normal(-0.0001, 0.006, n_days),    # æŠ•è³‡ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
                'Mom': np.random.normal(0.0003, 0.011, n_days),     # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
                'RF': np.full(n_days, 0.00008)                      # ãƒªã‚¹ã‚¯ãƒ•ãƒªãƒ¼ãƒ¬ãƒ¼ãƒˆï¼ˆç´„2%å¹´ç‡ï¼‰
            }
            
            # DataFrameã«å¤‰æ›
            ff_factors = pd.DataFrame(factor_data, index=date_range)
            
            logger.info(f"ã‚µãƒ³ãƒ—ãƒ«Fama-Frenchãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {len(ff_factors)}æ—¥åˆ†")
            
            return {'FF5_Factors': ff_factors}
            
        except Exception as e:
            logger.error(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
@st.cache_resource
def get_data_manager() -> DataManager:
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    """
    return DataManager()