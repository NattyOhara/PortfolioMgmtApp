"""
ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½
"""

import pandas as pd
import streamlit as st
from typing import Optional, Tuple, List
import logging

logger = logging.getLogger(__name__)


def load_portfolio_data(uploaded_file) -> Optional[pd.DataFrame]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        uploaded_file: Streamlitã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    
    Returns:
        pd.DataFrame: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ã®ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿
        None: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
    """
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        df = pd.read_csv(uploaded_file)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        validation_result, error_messages = validate_portfolio_data(df)
        
        if not validation_result:
            for error in error_messages:
                st.error(error)
            return None
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        df = clean_portfolio_data(df)
        
        logger.info(f"ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(df)}éŠ˜æŸ„")
        return df
        
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None


def validate_portfolio_data(df: pd.DataFrame) -> Tuple[bool, List[str]]:
    """
    ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    
    Args:
        df: æ¤œè¨¼ã™ã‚‹DataFrame
    
    Returns:
        Tuple[bool, List[str]]: (æ¤œè¨¼çµæœ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ)
    """
    errors = []
    
    # å¿…é ˆåˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    required_columns = ['Ticker', 'Shares', 'AvgCostJPY']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        errors.append(f"å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")
    
    # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
    if df.empty:
        errors.append("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return False, errors
    
    # åˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
    if 'Shares' in df.columns:
        if not pd.api.types.is_numeric_dtype(df['Shares']):
            errors.append("Sharesåˆ—ã¯æ•°å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
        elif (df['Shares'] <= 0).any():
            errors.append("Sharesåˆ—ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    
    if 'AvgCostJPY' in df.columns:
        if not pd.api.types.is_numeric_dtype(df['AvgCostJPY']):
            errors.append("AvgCostJPYåˆ—ã¯æ•°å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
        elif (df['AvgCostJPY'] <= 0).any():
            errors.append("AvgCostJPYåˆ—ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    
    # ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    if 'Ticker' in df.columns:
        duplicates = df['Ticker'].duplicated()
        if duplicates.any():
            duplicate_tickers = df.loc[duplicates, 'Ticker'].tolist()
            errors.append(f"é‡è¤‡ã™ã‚‹ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ãŒã‚ã‚Šã¾ã™: {duplicate_tickers}")
    
    # NaNå€¤ã®ãƒã‚§ãƒƒã‚¯
    if df.isnull().any().any():
        null_columns = df.columns[df.isnull().any()].tolist()
        errors.append(f"ä»¥ä¸‹ã®åˆ—ã«NaNå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {null_columns}")
    
    return len(errors) == 0, errors


def clean_portfolio_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    
    Args:
        df: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®DataFrame
    
    Returns:
        pd.DataFrame: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®DataFrame
    """
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
    cleaned_df = df.copy()
    
    # ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã®å¤§æ–‡å­—å¤‰æ›ã¨ç©ºç™½é™¤å»
    cleaned_df['Ticker'] = cleaned_df['Ticker'].astype(str).str.strip().str.upper()
    
    # æ•°å€¤åˆ—ã®å‹å¤‰æ›
    cleaned_df['Shares'] = pd.to_numeric(cleaned_df['Shares'], errors='coerce')
    cleaned_df['AvgCostJPY'] = pd.to_numeric(cleaned_df['AvgCostJPY'], errors='coerce')
    
    # NaNå€¤ã‚’å«ã‚€è¡Œã‚’é™¤å»
    cleaned_df = cleaned_df.dropna()
    
    # é‡è¤‡è¡Œã®é™¤å»ï¼ˆãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
    cleaned_df = cleaned_df.drop_duplicates(subset=['Ticker'], keep='first')
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
    cleaned_df = cleaned_df.reset_index(drop=True)
    
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {len(cleaned_df)}éŠ˜æŸ„")
    return cleaned_df


def get_sample_data() -> pd.DataFrame:
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Returns:
        pd.DataFrame: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    """
    sample_data = {
        'Ticker': ['AAPL', 'MSFT', '7203.T', 'ASML', 'TSLA'],
        'Shares': [100, 50, 1000, 20, 30],
        'AvgCostJPY': [15000, 25000, 800, 60000, 20000]
    }
    return pd.DataFrame(sample_data)


def export_portfolio_data(df: pd.DataFrame, filename: str = "portfolio_export.csv") -> bytes:
    """
    ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    
    Args:
        df: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹DataFrame
        filename: ãƒ•ã‚¡ã‚¤ãƒ«å
    
    Returns:
        bytes: CSVå½¢å¼ã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
    """
    return df.to_csv(index=False).encode('utf-8')


def display_data_summary(df: pd.DataFrame):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
    
    Args:
        df: è¡¨ç¤ºã™ã‚‹DataFrame
    """
    st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("éŠ˜æŸ„æ•°", len(df))
    
    with col2:
        total_shares = df['Shares'].sum()
        st.metric("ç·ä¿æœ‰æ ªæ•°", f"{total_shares:,.0f}")
    
    with col3:
        total_investment = (df['Shares'] * df['AvgCostJPY']).sum()
        st.metric("ç·æŠ•è³‡é¡", f"Â¥{total_investment:,.0f}")
    
    with col4:
        avg_cost = (df['Shares'] * df['AvgCostJPY']).sum() / df['Shares'].sum()
        st.metric("å¹³å‡å–å¾—å˜ä¾¡", f"Â¥{avg_cost:,.0f}")
    
    # è©³ç´°çµ±è¨ˆ
    with st.expander("ğŸ“ˆ è©³ç´°çµ±è¨ˆ"):
        st.dataframe(df.describe(), use_container_width=True)